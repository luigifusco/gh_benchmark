//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33053471
// Cuda compilation tools, release 12.2, V12.2.128
// Based on NVVM 7.0.1
//

.version 8.2
.target sm_90
.address_size 64

.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.extern .func __assertfail
(
	.param .b64 __assertfail_param_0,
	.param .b64 __assertfail_param_1,
	.param .b32 __assertfail_param_2,
	.param .b64 __assertfail_param_3,
	.param .b64 __assertfail_param_4
)
;
.global .align 4 .b8 _ZZN4cuda3std3__48__detail21__stronger_order_cudaEiiE7__xform[16] = {3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 3};
// _ZZ21gpu_clock_test_kernelPlS_E2gt has been demoted
// _ZZ21gpu_clock_test_kernelPlS_E2lt has been demoted
// _ZZ27device_read_kernel_detailedPdmmPlE12times_vector has been demoted
// _ZZ23device_read_kernel_syncPdmmPlS0_E6clocks has been demoted
// _ZZ24device_write_kernel_syncPdmmPlS0_E6clocks has been demoted
// _ZZ23device_copy_kernel_syncPdS_mmPlS0_E6clocks has been demoted
.global .align 1 .b8 __unnamed_1[85] = {118, 111, 105, 100, 32, 100, 101, 118, 105, 99, 101, 95, 114, 101, 97, 100, 95, 107, 101, 114, 110, 101, 108, 95, 98, 108, 111, 99, 107, 40, 117, 110, 115, 105, 103, 110, 101, 100, 32, 108, 111, 110, 103, 32, 42, 44, 32, 117, 110, 115, 105, 103, 110, 101, 100, 32, 108, 111, 110, 103, 44, 32, 117, 110, 115, 105, 103, 110, 101, 100, 32, 108, 111, 110, 103, 44, 32, 108, 111, 110, 103, 32, 42, 41};
// _ZZ24device_read_kernel_blockPmmmPlE6clocks has been demoted
// _ZZ25device_write_kernel_blockPdmmPlE6clocks has been demoted
// _ZZ24device_copy_kernel_blockPdS_mmPlE6clocks has been demoted
.global .align 1 .b8 _ZN37_INTERNAL_acda41d5_7_main_cu_a5ec78574cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN37_INTERNAL_acda41d5_7_main_cu_a5ec78574cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 $str$1[5] = {37, 108, 102, 32};
.global .align 1 .b8 $str$2[5] = {37, 108, 117, 32};
.global .align 1 .b8 $str$3[16] = {98, 108, 111, 99, 107, 73, 100, 120, 46, 120, 32, 61, 61, 32, 48};
.global .align 1 .b8 $str$4[20] = {114, 101, 97, 100, 95, 119, 114, 105, 116, 101, 95, 99, 111, 112, 121, 46, 104, 112, 112};
.global .align 1 .b8 $str$5[4] = {37, 117, 32};

.func _Z9dumb_copyPVhS0_m(
	.param .b64 _Z9dumb_copyPVhS0_m_param_0,
	.param .b64 _Z9dumb_copyPVhS0_m_param_1,
	.param .b64 _Z9dumb_copyPVhS0_m_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd5, [_Z9dumb_copyPVhS0_m_param_0];
	ld.param.u64 	%rd6, [_Z9dumb_copyPVhS0_m_param_1];
	ld.param.u64 	%rd7, [_Z9dumb_copyPVhS0_m_param_2];
	setp.eq.s64 	%p1, %rd7, 0;
	@%p1 bra 	$L__BB0_3;

	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u64 	%rd11, 0;

$L__BB0_2:
	add.s64 	%rd9, %rd1, %rd11;
	ld.volatile.global.u8 	%rs1, [%rd9];
	add.s64 	%rd10, %rd2, %rd11;
	st.volatile.global.u8 	[%rd10], %rs1;
	add.s64 	%rd11, %rd11, 64;
	setp.lt.u64 	%p2, %rd11, %rd7;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	ret;

}
	// .globl	_Z24clock_granularity_kernelPl
.visible .entry _Z24clock_granularity_kernelPl(
	.param .u64 _Z24clock_granularity_kernelPl_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .b64 	%rd<58>;


	ld.param.u64 	%rd7, [_Z24clock_granularity_kernelPl_param_0];
	cvta.to.global.u64 	%rd56, %rd7;
	mov.u64 	%rd57, 10000;

$L__BB1_1:
	// begin inline asm
	mov.u64 	%rd8, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd9, %clock64;
	// end inline asm
	sub.s64 	%rd40, %rd9, %rd8;
	st.global.u64 	[%rd56], %rd40;
	// begin inline asm
	mov.u64 	%rd10, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd11, %clock64;
	// end inline asm
	sub.s64 	%rd41, %rd11, %rd10;
	st.global.u64 	[%rd56+8], %rd41;
	// begin inline asm
	mov.u64 	%rd12, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd13, %clock64;
	// end inline asm
	sub.s64 	%rd42, %rd13, %rd12;
	st.global.u64 	[%rd56+16], %rd42;
	// begin inline asm
	mov.u64 	%rd14, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd15, %clock64;
	// end inline asm
	sub.s64 	%rd43, %rd15, %rd14;
	st.global.u64 	[%rd56+24], %rd43;
	// begin inline asm
	mov.u64 	%rd16, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd17, %clock64;
	// end inline asm
	sub.s64 	%rd44, %rd17, %rd16;
	st.global.u64 	[%rd56+32], %rd44;
	// begin inline asm
	mov.u64 	%rd18, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd19, %clock64;
	// end inline asm
	sub.s64 	%rd45, %rd19, %rd18;
	st.global.u64 	[%rd56+40], %rd45;
	// begin inline asm
	mov.u64 	%rd20, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd21, %clock64;
	// end inline asm
	sub.s64 	%rd46, %rd21, %rd20;
	st.global.u64 	[%rd56+48], %rd46;
	// begin inline asm
	mov.u64 	%rd22, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd23, %clock64;
	// end inline asm
	sub.s64 	%rd47, %rd23, %rd22;
	st.global.u64 	[%rd56+56], %rd47;
	// begin inline asm
	mov.u64 	%rd24, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd25, %clock64;
	// end inline asm
	sub.s64 	%rd48, %rd25, %rd24;
	st.global.u64 	[%rd56+64], %rd48;
	// begin inline asm
	mov.u64 	%rd26, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd27, %clock64;
	// end inline asm
	sub.s64 	%rd49, %rd27, %rd26;
	st.global.u64 	[%rd56+72], %rd49;
	// begin inline asm
	mov.u64 	%rd28, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd29, %clock64;
	// end inline asm
	sub.s64 	%rd50, %rd29, %rd28;
	st.global.u64 	[%rd56+80], %rd50;
	// begin inline asm
	mov.u64 	%rd30, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd31, %clock64;
	// end inline asm
	sub.s64 	%rd51, %rd31, %rd30;
	st.global.u64 	[%rd56+88], %rd51;
	// begin inline asm
	mov.u64 	%rd32, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd33, %clock64;
	// end inline asm
	sub.s64 	%rd52, %rd33, %rd32;
	st.global.u64 	[%rd56+96], %rd52;
	// begin inline asm
	mov.u64 	%rd34, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd35, %clock64;
	// end inline asm
	sub.s64 	%rd53, %rd35, %rd34;
	st.global.u64 	[%rd56+104], %rd53;
	// begin inline asm
	mov.u64 	%rd36, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd37, %clock64;
	// end inline asm
	sub.s64 	%rd54, %rd37, %rd36;
	st.global.u64 	[%rd56+112], %rd54;
	// begin inline asm
	mov.u64 	%rd38, %clock64;
	// end inline asm
	// begin inline asm
	mov.u64 	%rd39, %clock64;
	// end inline asm
	sub.s64 	%rd55, %rd39, %rd38;
	st.global.u64 	[%rd56+120], %rd55;
	add.s64 	%rd56, %rd56, 128;
	add.s64 	%rd57, %rd57, -16;
	setp.ne.s64 	%p1, %rd57, 0;
	@%p1 bra 	$L__BB1_1;

	ret;

}
	// .globl	_Z31global_clock_granularity_kernelPl
.visible .entry _Z31global_clock_granularity_kernelPl(
	.param .u64 _Z31global_clock_granularity_kernelPl_param_0
)
{
	.reg .pred 	%p<2>;
	.reg .b64 	%rd<58>;


	ld.param.u64 	%rd7, [_Z31global_clock_granularity_kernelPl_param_0];
	cvta.to.global.u64 	%rd56, %rd7;
	mov.u64 	%rd57, 10000;

$L__BB2_1:
	// begin inline asm
	mov.u64 %rd8, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd9, %globaltimer;
	// end inline asm
	sub.s64 	%rd40, %rd9, %rd8;
	st.global.u64 	[%rd56], %rd40;
	// begin inline asm
	mov.u64 %rd10, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd11, %globaltimer;
	// end inline asm
	sub.s64 	%rd41, %rd11, %rd10;
	st.global.u64 	[%rd56+8], %rd41;
	// begin inline asm
	mov.u64 %rd12, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd13, %globaltimer;
	// end inline asm
	sub.s64 	%rd42, %rd13, %rd12;
	st.global.u64 	[%rd56+16], %rd42;
	// begin inline asm
	mov.u64 %rd14, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd15, %globaltimer;
	// end inline asm
	sub.s64 	%rd43, %rd15, %rd14;
	st.global.u64 	[%rd56+24], %rd43;
	// begin inline asm
	mov.u64 %rd16, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd17, %globaltimer;
	// end inline asm
	sub.s64 	%rd44, %rd17, %rd16;
	st.global.u64 	[%rd56+32], %rd44;
	// begin inline asm
	mov.u64 %rd18, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd19, %globaltimer;
	// end inline asm
	sub.s64 	%rd45, %rd19, %rd18;
	st.global.u64 	[%rd56+40], %rd45;
	// begin inline asm
	mov.u64 %rd20, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd21, %globaltimer;
	// end inline asm
	sub.s64 	%rd46, %rd21, %rd20;
	st.global.u64 	[%rd56+48], %rd46;
	// begin inline asm
	mov.u64 %rd22, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd23, %globaltimer;
	// end inline asm
	sub.s64 	%rd47, %rd23, %rd22;
	st.global.u64 	[%rd56+56], %rd47;
	// begin inline asm
	mov.u64 %rd24, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd25, %globaltimer;
	// end inline asm
	sub.s64 	%rd48, %rd25, %rd24;
	st.global.u64 	[%rd56+64], %rd48;
	// begin inline asm
	mov.u64 %rd26, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd27, %globaltimer;
	// end inline asm
	sub.s64 	%rd49, %rd27, %rd26;
	st.global.u64 	[%rd56+72], %rd49;
	// begin inline asm
	mov.u64 %rd28, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd29, %globaltimer;
	// end inline asm
	sub.s64 	%rd50, %rd29, %rd28;
	st.global.u64 	[%rd56+80], %rd50;
	// begin inline asm
	mov.u64 %rd30, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd31, %globaltimer;
	// end inline asm
	sub.s64 	%rd51, %rd31, %rd30;
	st.global.u64 	[%rd56+88], %rd51;
	// begin inline asm
	mov.u64 %rd32, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd33, %globaltimer;
	// end inline asm
	sub.s64 	%rd52, %rd33, %rd32;
	st.global.u64 	[%rd56+96], %rd52;
	// begin inline asm
	mov.u64 %rd34, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd35, %globaltimer;
	// end inline asm
	sub.s64 	%rd53, %rd35, %rd34;
	st.global.u64 	[%rd56+104], %rd53;
	// begin inline asm
	mov.u64 %rd36, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd37, %globaltimer;
	// end inline asm
	sub.s64 	%rd54, %rd37, %rd36;
	st.global.u64 	[%rd56+112], %rd54;
	// begin inline asm
	mov.u64 %rd38, %globaltimer;
	// end inline asm
	// begin inline asm
	mov.u64 %rd39, %globaltimer;
	// end inline asm
	sub.s64 	%rd55, %rd39, %rd38;
	st.global.u64 	[%rd56+120], %rd55;
	add.s64 	%rd56, %rd56, 128;
	add.s64 	%rd57, %rd57, -16;
	setp.ne.s64 	%p1, %rd57, 0;
	@%p1 bra 	$L__BB2_1;

	ret;

}
	// .globl	_Z26basic_loop_overhead_kernelmPlPm
.visible .entry _Z26basic_loop_overhead_kernelmPlPm(
	.param .u64 _Z26basic_loop_overhead_kernelmPlPm_param_0,
	.param .u64 _Z26basic_loop_overhead_kernelmPlPm_param_1,
	.param .u64 _Z26basic_loop_overhead_kernelmPlPm_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .b64 	%rd<45>;


	ld.param.u64 	%rd18, [_Z26basic_loop_overhead_kernelmPlPm_param_0];
	ld.param.u64 	%rd19, [_Z26basic_loop_overhead_kernelmPlPm_param_1];
	ld.param.u64 	%rd20, [_Z26basic_loop_overhead_kernelmPlPm_param_2];
	// begin inline asm
	mov.u64 	%rd21, %clock64;
	// end inline asm
	setp.eq.s64 	%p1, %rd18, 0;
	@%p1 bra 	$L__BB3_7;

	add.s64 	%rd25, %rd18, -1;
	and.b64  	%rd2, %rd18, 3;
	setp.lt.u64 	%p2, %rd25, 3;
	mov.u64 	%rd40, 0;
	@%p2 bra 	$L__BB3_4;

	sub.s64 	%rd3, %rd2, %rd18;

$L__BB3_3:
	add.s64 	%rd28, %rd40, %rd44;
	add.s64 	%rd29, %rd40, %rd28;
	add.s64 	%rd30, %rd40, %rd29;
	add.s64 	%rd31, %rd40, %rd30;
	add.s64 	%rd44, %rd31, 6;
	add.s64 	%rd40, %rd40, 4;
	add.s64 	%rd32, %rd3, %rd40;
	setp.ne.s64 	%p3, %rd32, 0;
	@%p3 bra 	$L__BB3_3;

$L__BB3_4:
	setp.eq.s64 	%p4, %rd2, 0;
	@%p4 bra 	$L__BB3_7;

	neg.s64 	%rd41, %rd2;

$L__BB3_6:
	.pragma "nounroll";
	add.s64 	%rd44, %rd40, %rd44;
	add.s64 	%rd40, %rd40, 1;
	add.s64 	%rd41, %rd41, 1;
	setp.ne.s64 	%p5, %rd41, 0;
	@%p5 bra 	$L__BB3_6;

$L__BB3_7:
	// begin inline asm
	mov.u64 	%rd33, %clock64;
	// end inline asm
	sub.s64 	%rd34, %rd33, %rd21;
	cvta.to.global.u64 	%rd35, %rd19;
	cvta.to.global.u64 	%rd36, %rd20;
	st.global.u64 	[%rd35], %rd34;
	st.global.u64 	[%rd36], %rd44;
	ret;

}
	// .globl	_Z21gpu_clock_test_kernelPlS_
.visible .entry _Z21gpu_clock_test_kernelPlS_(
	.param .u64 _Z21gpu_clock_test_kernelPlS__param_0,
	.param .u64 _Z21gpu_clock_test_kernelPlS__param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<87>;
	// demoted variable
	.shared .align 8 .b8 _ZZ21gpu_clock_test_kernelPlS_E2gt[8192];
	// demoted variable
	.shared .align 8 .b8 _ZZ21gpu_clock_test_kernelPlS_E2lt[8192];

	ld.param.u64 	%rd12, [_Z21gpu_clock_test_kernelPlS__param_0];
	ld.param.u64 	%rd14, [_Z21gpu_clock_test_kernelPlS__param_1];
	cvta.to.global.u64 	%rd1, %rd14;
	mov.u64 	%rd83, 1024;
	mov.u32 	%r16, _ZZ21gpu_clock_test_kernelPlS_E2gt;
	mov.u32 	%r15, _ZZ21gpu_clock_test_kernelPlS_E2lt;

$L__BB4_1:
	// begin inline asm
	mov.u64 %rd15, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16], %rd15;
	// begin inline asm
	mov.u64 	%rd16, %clock64;
	// end inline asm
	st.shared.u64 	[%r15], %rd16;
	// begin inline asm
	mov.u64 %rd17, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+8], %rd17;
	// begin inline asm
	mov.u64 	%rd18, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+8], %rd18;
	// begin inline asm
	mov.u64 %rd19, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+16], %rd19;
	// begin inline asm
	mov.u64 	%rd20, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+16], %rd20;
	// begin inline asm
	mov.u64 %rd21, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+24], %rd21;
	// begin inline asm
	mov.u64 	%rd22, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+24], %rd22;
	// begin inline asm
	mov.u64 %rd23, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+32], %rd23;
	// begin inline asm
	mov.u64 	%rd24, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+32], %rd24;
	// begin inline asm
	mov.u64 %rd25, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+40], %rd25;
	// begin inline asm
	mov.u64 	%rd26, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+40], %rd26;
	// begin inline asm
	mov.u64 %rd27, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+48], %rd27;
	// begin inline asm
	mov.u64 	%rd28, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+48], %rd28;
	// begin inline asm
	mov.u64 %rd29, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+56], %rd29;
	// begin inline asm
	mov.u64 	%rd30, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+56], %rd30;
	// begin inline asm
	mov.u64 %rd31, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+64], %rd31;
	// begin inline asm
	mov.u64 	%rd32, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+64], %rd32;
	// begin inline asm
	mov.u64 %rd33, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+72], %rd33;
	// begin inline asm
	mov.u64 	%rd34, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+72], %rd34;
	// begin inline asm
	mov.u64 %rd35, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+80], %rd35;
	// begin inline asm
	mov.u64 	%rd36, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+80], %rd36;
	// begin inline asm
	mov.u64 %rd37, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+88], %rd37;
	// begin inline asm
	mov.u64 	%rd38, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+88], %rd38;
	// begin inline asm
	mov.u64 %rd39, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+96], %rd39;
	// begin inline asm
	mov.u64 	%rd40, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+96], %rd40;
	// begin inline asm
	mov.u64 %rd41, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+104], %rd41;
	// begin inline asm
	mov.u64 	%rd42, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+104], %rd42;
	// begin inline asm
	mov.u64 %rd43, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+112], %rd43;
	// begin inline asm
	mov.u64 	%rd44, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+112], %rd44;
	// begin inline asm
	mov.u64 %rd45, %globaltimer;
	// end inline asm
	st.shared.u64 	[%r16+120], %rd45;
	// begin inline asm
	mov.u64 	%rd46, %clock64;
	// end inline asm
	st.shared.u64 	[%r15+120], %rd46;
	add.s32 	%r16, %r16, 128;
	add.s32 	%r15, %r15, 128;
	add.s64 	%rd83, %rd83, -16;
	setp.ne.s64 	%p1, %rd83, 0;
	@%p1 bra 	$L__BB4_1;

	cvta.to.global.u64 	%rd48, %rd12;
	mov.u32 	%r13, %ctaid.x;
	shl.b32 	%r14, %r13, 10;
	mul.wide.u32 	%rd49, %r14, 8;
	or.b64  	%rd50, %rd49, 64;
	add.s64 	%rd85, %rd1, %rd50;
	add.s64 	%rd84, %rd48, %rd50;
	mov.u64 	%rd86, 1024;
	mov.u32 	%r18, _ZZ21gpu_clock_test_kernelPlS_E2lt;
	mov.u32 	%r17, _ZZ21gpu_clock_test_kernelPlS_E2gt;

$L__BB4_3:
	ld.shared.u64 	%rd51, [%r17];
	st.global.u64 	[%rd84+-64], %rd51;
	ld.shared.u64 	%rd52, [%r18];
	st.global.u64 	[%rd85+-64], %rd52;
	ld.shared.u64 	%rd53, [%r17+8];
	st.global.u64 	[%rd84+-56], %rd53;
	ld.shared.u64 	%rd54, [%r18+8];
	st.global.u64 	[%rd85+-56], %rd54;
	ld.shared.u64 	%rd55, [%r17+16];
	st.global.u64 	[%rd84+-48], %rd55;
	ld.shared.u64 	%rd56, [%r18+16];
	st.global.u64 	[%rd85+-48], %rd56;
	ld.shared.u64 	%rd57, [%r17+24];
	st.global.u64 	[%rd84+-40], %rd57;
	ld.shared.u64 	%rd58, [%r18+24];
	st.global.u64 	[%rd85+-40], %rd58;
	ld.shared.u64 	%rd59, [%r17+32];
	st.global.u64 	[%rd84+-32], %rd59;
	ld.shared.u64 	%rd60, [%r18+32];
	st.global.u64 	[%rd85+-32], %rd60;
	ld.shared.u64 	%rd61, [%r17+40];
	st.global.u64 	[%rd84+-24], %rd61;
	ld.shared.u64 	%rd62, [%r18+40];
	st.global.u64 	[%rd85+-24], %rd62;
	ld.shared.u64 	%rd63, [%r17+48];
	st.global.u64 	[%rd84+-16], %rd63;
	ld.shared.u64 	%rd64, [%r18+48];
	st.global.u64 	[%rd85+-16], %rd64;
	ld.shared.u64 	%rd65, [%r17+56];
	st.global.u64 	[%rd84+-8], %rd65;
	ld.shared.u64 	%rd66, [%r18+56];
	st.global.u64 	[%rd85+-8], %rd66;
	ld.shared.u64 	%rd67, [%r17+64];
	st.global.u64 	[%rd84], %rd67;
	ld.shared.u64 	%rd68, [%r18+64];
	st.global.u64 	[%rd85], %rd68;
	ld.shared.u64 	%rd69, [%r17+72];
	st.global.u64 	[%rd84+8], %rd69;
	ld.shared.u64 	%rd70, [%r18+72];
	st.global.u64 	[%rd85+8], %rd70;
	ld.shared.u64 	%rd71, [%r17+80];
	st.global.u64 	[%rd84+16], %rd71;
	ld.shared.u64 	%rd72, [%r18+80];
	st.global.u64 	[%rd85+16], %rd72;
	ld.shared.u64 	%rd73, [%r17+88];
	st.global.u64 	[%rd84+24], %rd73;
	ld.shared.u64 	%rd74, [%r18+88];
	st.global.u64 	[%rd85+24], %rd74;
	ld.shared.u64 	%rd75, [%r17+96];
	st.global.u64 	[%rd84+32], %rd75;
	ld.shared.u64 	%rd76, [%r18+96];
	st.global.u64 	[%rd85+32], %rd76;
	ld.shared.u64 	%rd77, [%r17+104];
	st.global.u64 	[%rd84+40], %rd77;
	ld.shared.u64 	%rd78, [%r18+104];
	st.global.u64 	[%rd85+40], %rd78;
	ld.shared.u64 	%rd79, [%r17+112];
	st.global.u64 	[%rd84+48], %rd79;
	ld.shared.u64 	%rd80, [%r18+112];
	st.global.u64 	[%rd85+48], %rd80;
	ld.shared.u64 	%rd81, [%r17+120];
	st.global.u64 	[%rd84+56], %rd81;
	ld.shared.u64 	%rd82, [%r18+120];
	st.global.u64 	[%rd85+56], %rd82;
	add.s64 	%rd85, %rd85, 128;
	add.s64 	%rd84, %rd84, 128;
	add.s32 	%r18, %r18, 128;
	add.s32 	%r17, %r17, 128;
	add.s64 	%rd86, %rd86, -16;
	setp.ne.s64 	%p2, %rd86, 0;
	@%p2 bra 	$L__BB4_3;

	ret;

}
	// .globl	_Z31device_write_preparation_kernelPhm
.visible .entry _Z31device_write_preparation_kernelPhm(
	.param .u64 _Z31device_write_preparation_kernelPhm_param_0,
	.param .u64 _Z31device_write_preparation_kernelPhm_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd6, [_Z31device_write_preparation_kernelPhm_param_0];
	ld.param.u64 	%rd7, [_Z31device_write_preparation_kernelPhm_param_1];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	mul.wide.u32 	%rd9, %r4, 32;
	setp.ge.u64 	%p1, %rd9, %rd7;
	@%p1 bra 	$L__BB5_3;

	mov.u32 	%r5, %nctaid.x;
	mul.lo.s32 	%r6, %r5, %r1;
	shl.b32 	%r7, %r6, 5;
	cvt.u64.u32 	%rd2, %r7;
	cvta.to.global.u64 	%rd3, %rd6;

$L__BB5_2:
	add.s64 	%rd8, %rd3, %rd9;
	mov.u16 	%rs1, 0;
	st.global.u8 	[%rd8], %rs1;
	add.s64 	%rd9, %rd9, %rd2;
	setp.lt.u64 	%p2, %rd9, %rd7;
	@%p2 bra 	$L__BB5_2;

$L__BB5_3:
	ret;

}
	// .globl	_Z30device_read_preparation_kernelPhm
.visible .entry _Z30device_read_preparation_kernelPhm(
	.param .u64 _Z30device_read_preparation_kernelPhm_param_0,
	.param .u64 _Z30device_read_preparation_kernelPhm_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd5, [_Z30device_read_preparation_kernelPhm_param_1];
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r3, %r1, %r2;
	mul.wide.u32 	%rd6, %r4, 32;
	setp.ge.u64 	%p1, %rd6, %rd5;
	@%p1 bra 	$L__BB6_3;

	mov.u32 	%r5, %nctaid.x;
	mul.lo.s32 	%r6, %r5, %r1;
	shl.b32 	%r7, %r6, 5;
	cvt.u64.u32 	%rd2, %r7;

$L__BB6_2:
	add.s64 	%rd6, %rd6, %rd2;
	setp.lt.u64 	%p2, %rd6, %rd5;
	@%p2 bra 	$L__BB6_2;

$L__BB6_3:
	ret;

}
	// .globl	_Z27device_modified_init_kernelPhm
.visible .entry _Z27device_modified_init_kernelPhm(
	.param .u64 _Z27device_modified_init_kernelPhm_param_0,
	.param .u64 _Z27device_modified_init_kernelPhm_param_1
)
{
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [_Z27device_modified_init_kernelPhm_param_0];
	ld.param.u64 	%rd2, [_Z27device_modified_init_kernelPhm_param_1];
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd1;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd1;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd2;
	call.uni 
	_Z9dumb_copyPVhS0_m, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 0
	ret;

}
	// .globl	_Z24loopy_write_kernel_clockPhmPlS0_
.visible .entry _Z24loopy_write_kernel_clockPhmPlS0_(
	.param .u64 _Z24loopy_write_kernel_clockPhmPlS0__param_0,
	.param .u64 _Z24loopy_write_kernel_clockPhmPlS0__param_1,
	.param .u64 _Z24loopy_write_kernel_clockPhmPlS0__param_2,
	.param .u64 _Z24loopy_write_kernel_clockPhmPlS0__param_3
)
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<39>;


	ld.param.u64 	%rd36, [_Z24loopy_write_kernel_clockPhmPlS0__param_0];
	ld.param.u64 	%rd19, [_Z24loopy_write_kernel_clockPhmPlS0__param_1];
	ld.param.u64 	%rd16, [_Z24loopy_write_kernel_clockPhmPlS0__param_2];
	ld.param.u64 	%rd17, [_Z24loopy_write_kernel_clockPhmPlS0__param_3];
	// begin inline asm
	mov.u64 	%rd18, %clock64;
	// end inline asm
	shr.u64 	%rd2, %rd19, 6;
	setp.eq.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB8_7;

	add.s64 	%rd20, %rd2, -1;
	and.b64  	%rd3, %rd2, 7;
	setp.lt.u64 	%p2, %rd20, 7;
	@%p2 bra 	$L__BB8_4;

	sub.s64 	%rd34, %rd3, %rd2;

$L__BB8_3:
	.pragma "nounroll";
	ld.u64 	%rd21, [%rd36];
	mov.u16 	%rs1, 0;
	st.volatile.u8 	[%rd36], %rs1;
	ld.u64 	%rd22, [%rd21];
	st.volatile.u8 	[%rd21], %rs1;
	ld.u64 	%rd23, [%rd22];
	st.volatile.u8 	[%rd22], %rs1;
	ld.u64 	%rd24, [%rd23];
	st.volatile.u8 	[%rd23], %rs1;
	ld.u64 	%rd25, [%rd24];
	st.volatile.u8 	[%rd24], %rs1;
	ld.u64 	%rd26, [%rd25];
	st.volatile.u8 	[%rd25], %rs1;
	ld.u64 	%rd27, [%rd26];
	st.volatile.u8 	[%rd26], %rs1;
	ld.u64 	%rd36, [%rd27];
	st.volatile.u8 	[%rd27], %rs1;
	add.s64 	%rd34, %rd34, 8;
	setp.ne.s64 	%p3, %rd34, 0;
	@%p3 bra 	$L__BB8_3;

$L__BB8_4:
	setp.eq.s64 	%p4, %rd3, 0;
	@%p4 bra 	$L__BB8_7;

	neg.s64 	%rd37, %rd3;

$L__BB8_6:
	.pragma "nounroll";
	ld.u64 	%rd13, [%rd36];
	mov.u16 	%rs2, 0;
	st.volatile.u8 	[%rd36], %rs2;
	add.s64 	%rd37, %rd37, 1;
	setp.ne.s64 	%p5, %rd37, 0;
	mov.u64 	%rd36, %rd13;
	@%p5 bra 	$L__BB8_6;

$L__BB8_7:
	membar.sys;
	// begin inline asm
	mov.u64 	%rd28, %clock64;
	// end inline asm
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mad.lo.s32 	%r4, %r2, %r3, %r1;
	cvta.to.global.u64 	%rd29, %rd16;
	mul.wide.s32 	%rd30, %r4, 8;
	add.s64 	%rd31, %rd29, %rd30;
	cvta.to.global.u64 	%rd32, %rd17;
	add.s64 	%rd33, %rd32, %rd30;
	st.global.u64 	[%rd31], %rd18;
	st.global.u64 	[%rd33], %rd28;
	ret;

}
	// .globl	_Z20pointer_chase_kernelPy
.visible .entry _Z20pointer_chase_kernelPy(
	.param .u64 _Z20pointer_chase_kernelPy_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd5, [_Z20pointer_chase_kernelPy_param_0];
	setp.eq.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB9_2;

$L__BB9_1:
	cvta.to.global.u64 	%rd4, %rd5;
	ld.global.nc.u64 	%rd5, [%rd4];
	setp.ne.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB9_1;

$L__BB9_2:
	ret;

}
	// .globl	_Z31atomic_cas_pointer_chase_kernelPy
.visible .entry _Z31atomic_cas_pointer_chase_kernelPy(
	.param .u64 _Z31atomic_cas_pointer_chase_kernelPy_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd5, [_Z31atomic_cas_pointer_chase_kernelPy_param_0];
	setp.eq.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB10_2;

$L__BB10_1:
	mov.u64 	%rd4, 0;
	atom.cas.b64 	%rd5, [%rd5], %rd4, %rd4;
	setp.ne.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB10_1;

$L__BB10_2:
	ret;

}
	// .globl	_Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m
.visible .entry _Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m(
	.param .u64 _Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_0,
	.param .u64 _Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_1,
	.param .u64 _Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_2,
	.param .u64 _Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_3,
	.param .u64 _Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_4,
	.param .u64 _Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_5,
	.param .u64 _Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_6
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd12, [_Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_0];
	ld.param.u64 	%rd13, [_Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_1];
	ld.param.u64 	%rd14, [_Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_2];
	ld.param.u64 	%rd15, [_Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_3];
	ld.param.u64 	%rd16, [_Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_4];
	ld.param.u64 	%rd17, [_Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_5];
	ld.param.u64 	%rd18, [_Z30ping_pong_receive_first_kernelPvS_S_PbS_S0_m_param_6];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r4, %tid.z;
	mov.u32 	%r5, %tid.y;
	mad.lo.s32 	%r6, %r1, %r4, %r5;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r3, %r6, %r2, %r7;
	setp.ne.s32 	%p1, %r3, 0;
	@%p1 bra 	$L__BB11_2;

	cvta.to.global.u64 	%rd20, %rd17;
	ld.global.u8 	%rs1, [%rd20];
	setp.eq.s16 	%p2, %rs1, 0;
	@%p2 bra 	$L__BB11_9;

$L__BB11_2:
	bar.sync 	0;
	shr.u64 	%rd1, %rd18, 3;
	cvt.u64.u32 	%rd33, %r3;
	setp.le.u64 	%p3, %rd1, %rd33;
	@%p3 bra 	$L__BB11_6;

	mul.lo.s32 	%r8, %r2, %r1;
	mov.u32 	%r9, %ntid.z;
	mul.lo.s32 	%r10, %r8, %r9;
	cvt.u64.u32 	%rd3, %r10;
	cvta.to.global.u64 	%rd4, %rd13;
	cvta.to.global.u64 	%rd5, %rd16;
	cvta.to.global.u64 	%rd6, %rd14;
	cvta.to.global.u64 	%rd7, %rd12;
	mov.u64 	%rd32, %rd33;

$L__BB11_4:
	shl.b64 	%rd22, %rd32, 3;
	add.s64 	%rd23, %rd5, %rd22;
	ld.global.u64 	%rd24, [%rd23];
	add.s64 	%rd25, %rd4, %rd22;
	st.global.u64 	[%rd25], %rd24;
	add.s64 	%rd32, %rd32, %rd3;
	setp.lt.u64 	%p4, %rd32, %rd1;
	@%p4 bra 	$L__BB11_4;

$L__BB11_5:
	shl.b64 	%rd27, %rd33, 3;
	add.s64 	%rd28, %rd7, %rd27;
	ld.global.u64 	%rd29, [%rd28];
	add.s64 	%rd30, %rd6, %rd27;
	st.global.u64 	[%rd30], %rd29;
	add.s64 	%rd33, %rd33, %rd3;
	setp.lt.u64 	%p5, %rd33, %rd1;
	@%p5 bra 	$L__BB11_5;

$L__BB11_6:
	membar.sys;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_8;

	cvta.to.global.u64 	%rd31, %rd15;
	mov.u16 	%rs2, 1;
	st.global.u8 	[%rd31], %rs2;

$L__BB11_8:
	ret;

$L__BB11_9:
	bra.uni 	$L__BB11_9;

}
	// .globl	_Z19dynamic_work_kernelv
.visible .entry _Z19dynamic_work_kernelv()
{



	ret;

}
	// .globl	_Z28flattened_parallelism_kerneliiN3cub11GridBarrierE
.visible .entry _Z28flattened_parallelism_kerneliiN3cub11GridBarrierE(
	.param .u32 _Z28flattened_parallelism_kerneliiN3cub11GridBarrierE_param_0,
	.param .u32 _Z28flattened_parallelism_kerneliiN3cub11GridBarrierE_param_1,
	.param .align 8 .b8 _Z28flattened_parallelism_kerneliiN3cub11GridBarrierE_param_2[8]
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [_Z28flattened_parallelism_kerneliiN3cub11GridBarrierE_param_2];
	cvta.to.global.u64 	%rd2, %rd1;
	membar.gl;
	bar.sync 	0;
	mov.u32 	%r1, %ctaid.x;
	setp.eq.s32 	%p1, %r1, 0;
	mov.u32 	%r18, %tid.x;
	@%p1 bra 	$L__BB13_5;

	setp.ne.s32 	%p2, %r18, 0;
	@%p2 bra 	$L__BB13_4;

	mul.wide.u32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd2, %rd6;
	mov.u32 	%r11, 1;
	st.volatile.global.u32 	[%rd7], %r11;
	add.s64 	%rd5, %rd1, %rd6;
	// begin inline asm
	ld.cg.u32 %r10, [%rd5];
	// end inline asm
	setp.ne.s32 	%p3, %r10, 1;
	@%p3 bra 	$L__BB13_4;

$L__BB13_3:
	membar.cta;
	// begin inline asm
	ld.cg.u32 %r12, [%rd5];
	// end inline asm
	setp.eq.s32 	%p4, %r12, 1;
	@%p4 bra 	$L__BB13_3;

$L__BB13_4:
	bar.sync 	0;
	bra.uni 	$L__BB13_15;

$L__BB13_5:
	setp.ne.s32 	%p5, %r18, 0;
	@%p5 bra 	$L__BB13_7;

	mov.u32 	%r13, 1;
	st.volatile.global.u32 	[%rd2], %r13;

$L__BB13_7:
	bar.sync 	0;
	mov.u32 	%r3, %nctaid.x;
	setp.ge.u32 	%p6, %r18, %r3;
	@%p6 bra 	$L__BB13_12;

	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r17, %r18;

$L__BB13_9:
	mul.wide.s32 	%rd10, %r17, 4;
	add.s64 	%rd9, %rd1, %rd10;
	// begin inline asm
	ld.cg.u32 %r14, [%rd9];
	// end inline asm
	setp.ne.s32 	%p7, %r14, 0;
	@%p7 bra 	$L__BB13_11;

$L__BB13_10:
	membar.cta;
	// begin inline asm
	ld.cg.u32 %r15, [%rd9];
	// end inline asm
	setp.eq.s32 	%p8, %r15, 0;
	@%p8 bra 	$L__BB13_10;

$L__BB13_11:
	add.s32 	%r17, %r17, %r4;
	setp.lt.u32 	%p9, %r17, %r3;
	@%p9 bra 	$L__BB13_9;

$L__BB13_12:
	bar.sync 	0;
	@%p6 bra 	$L__BB13_15;

	mov.u32 	%r7, %ntid.x;

$L__BB13_14:
	mul.wide.s32 	%rd12, %r18, 4;
	add.s64 	%rd13, %rd2, %rd12;
	mov.u32 	%r16, 0;
	st.volatile.global.u32 	[%rd13], %r16;
	add.s32 	%r18, %r18, %r7;
	setp.lt.u32 	%p11, %r18, %r3;
	@%p11 bra 	$L__BB13_14;

$L__BB13_15:
	ret;

}
	// .globl	_Z30cooperative_parallelism_kernelii
.visible .entry _Z30cooperative_parallelism_kernelii(
	.param .u32 _Z30cooperative_parallelism_kernelii_param_0,
	.param .u32 _Z30cooperative_parallelism_kernelii_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<55>;
	.reg .b64 	%rd<7>;


	ld.param.u32 	%r12, [_Z30cooperative_parallelism_kernelii_param_0];
	ld.param.u32 	%r13, [_Z30cooperative_parallelism_kernelii_param_1];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r54, %r1, %r2, %r3;
	// begin inline asm
	mov.u32 %r14, %envreg2;
	// end inline asm
	cvt.u64.u32 	%rd3, %r14;
	// begin inline asm
	mov.u32 %r15, %envreg1;
	// end inline asm
	cvt.u64.u32 	%rd4, %r15;
	bfi.b64 	%rd1, %rd4, %rd3, 32, 32;
	setp.ge.s32 	%p1, %r54, %r12;
	@%p1 bra 	$L__BB14_3;

	mov.u32 	%r16, %nctaid.x;
	mov.u32 	%r17, %nctaid.z;
	mov.u32 	%r18, %nctaid.y;
	mul.lo.s32 	%r19, %r18, %r17;
	mul.lo.s32 	%r20, %r19, %r16;
	mov.u32 	%r21, %ntid.y;
	mul.lo.s32 	%r22, %r2, %r21;
	mov.u32 	%r23, %ntid.z;
	mul.lo.s32 	%r24, %r22, %r23;
	mul.lo.s32 	%r5, %r20, %r24;
	mov.u32 	%r53, %r54;

$L__BB14_2:
	add.s32 	%r53, %r53, %r5;
	setp.lt.s32 	%p2, %r53, %r12;
	@%p2 bra 	$L__BB14_2;

$L__BB14_3:
	setp.ne.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB14_5;

	// begin inline asm
	trap;
	// end inline asm

$L__BB14_5:
	barrier.sync 	0;
	mov.u32 	%r25, %tid.y;
	add.s32 	%r26, %r3, %r25;
	mov.u32 	%r27, %tid.z;
	neg.s32 	%r28, %r27;
	setp.ne.s32 	%p4, %r26, %r28;
	@%p4 bra 	$L__BB14_8;

	add.s64 	%rd5, %rd1, 4;
	mov.u32 	%r31, %nctaid.x;
	mov.u32 	%r32, %nctaid.y;
	mul.lo.s32 	%r33, %r31, %r32;
	mov.u32 	%r34, %nctaid.z;
	mul.lo.s32 	%r35, %r33, %r34;
	mov.u32 	%r36, %ctaid.y;
	add.s32 	%r37, %r1, %r36;
	mov.u32 	%r38, %ctaid.z;
	neg.s32 	%r39, %r38;
	setp.eq.s32 	%p5, %r37, %r39;
	mov.u32 	%r40, -2147483647;
	sub.s32 	%r41, %r40, %r35;
	selp.b32 	%r30, %r41, 1, %p5;
	// begin inline asm
	atom.add.release.gpu.u32 %r29,[%rd5],%r30;
	// end inline asm

$L__BB14_7:
	// begin inline asm
	ld.acquire.gpu.u32 %r42,[%rd5];
	// end inline asm
	xor.b32  	%r43, %r42, %r29;
	setp.gt.s32 	%p6, %r43, -1;
	@%p6 bra 	$L__BB14_7;

$L__BB14_8:
	barrier.sync 	0;
	setp.ge.s32 	%p7, %r54, %r13;
	@%p7 bra 	$L__BB14_11;

	mov.u32 	%r44, %nctaid.x;
	mov.u32 	%r45, %nctaid.z;
	mov.u32 	%r46, %nctaid.y;
	mul.lo.s32 	%r47, %r46, %r45;
	mul.lo.s32 	%r48, %r47, %r44;
	mov.u32 	%r49, %ntid.y;
	mul.lo.s32 	%r50, %r2, %r49;
	mov.u32 	%r51, %ntid.z;
	mul.lo.s32 	%r52, %r50, %r51;
	mul.lo.s32 	%r9, %r48, %r52;

$L__BB14_10:
	add.s32 	%r54, %r54, %r9;
	setp.lt.s32 	%p8, %r54, %r13;
	@%p8 bra 	$L__BB14_10;

$L__BB14_11:
	ret;

}
	// .globl	_Z18device_pong_kernelRN4cuda3std3__46atomicIiEE
.visible .entry _Z18device_pong_kernelRN4cuda3std3__46atomicIiEE(
	.param .u64 _Z18device_pong_kernelRN4cuda3std3__46atomicIiEE_param_0
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd1, [_Z18device_pong_kernelRN4cuda3std3__46atomicIiEE_param_0];
	mov.u32 	%r25, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r25;
	// end inline asm

$L__BB15_1:
	// begin inline asm
	ld.acquire.sys.b32 %r5,[%rd1];
	// end inline asm
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB15_1;

	mov.u32 	%r6, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r6;
	// end inline asm

$L__BB15_3:
	// begin inline asm
	ld.acquire.sys.b32 %r7,[%rd1];
	// end inline asm
	setp.eq.s32 	%p2, %r7, 0;
	@%p2 bra 	$L__BB15_3;

	mov.u32 	%r8, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r8;
	// end inline asm

$L__BB15_5:
	// begin inline asm
	ld.acquire.sys.b32 %r9,[%rd1];
	// end inline asm
	setp.eq.s32 	%p3, %r9, 0;
	@%p3 bra 	$L__BB15_5;

	mov.u32 	%r10, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r10;
	// end inline asm

$L__BB15_7:
	// begin inline asm
	ld.acquire.sys.b32 %r11,[%rd1];
	// end inline asm
	setp.eq.s32 	%p4, %r11, 0;
	@%p4 bra 	$L__BB15_7;

	mov.u32 	%r12, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r12;
	// end inline asm

$L__BB15_9:
	// begin inline asm
	ld.acquire.sys.b32 %r13,[%rd1];
	// end inline asm
	setp.eq.s32 	%p5, %r13, 0;
	@%p5 bra 	$L__BB15_9;

	mov.u32 	%r14, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r14;
	// end inline asm

$L__BB15_11:
	// begin inline asm
	ld.acquire.sys.b32 %r15,[%rd1];
	// end inline asm
	setp.eq.s32 	%p6, %r15, 0;
	@%p6 bra 	$L__BB15_11;

	mov.u32 	%r16, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r16;
	// end inline asm

$L__BB15_13:
	// begin inline asm
	ld.acquire.sys.b32 %r17,[%rd1];
	// end inline asm
	setp.eq.s32 	%p7, %r17, 0;
	@%p7 bra 	$L__BB15_13;

	mov.u32 	%r18, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r18;
	// end inline asm

$L__BB15_15:
	// begin inline asm
	ld.acquire.sys.b32 %r19,[%rd1];
	// end inline asm
	setp.eq.s32 	%p8, %r19, 0;
	@%p8 bra 	$L__BB15_15;

	mov.u32 	%r20, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r20;
	// end inline asm

$L__BB15_17:
	// begin inline asm
	ld.acquire.sys.b32 %r21,[%rd1];
	// end inline asm
	setp.eq.s32 	%p9, %r21, 0;
	@%p9 bra 	$L__BB15_17;

	mov.u32 	%r22, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r22;
	// end inline asm

$L__BB15_19:
	// begin inline asm
	ld.acquire.sys.b32 %r23,[%rd1];
	// end inline asm
	setp.eq.s32 	%p10, %r23, 0;
	@%p10 bra 	$L__BB15_19;

	mov.u32 	%r24, 0;
	// begin inline asm
	st.release.sys.b32 [%rd1], %r24;
	// end inline asm
	add.s32 	%r25, %r25, 1;
	setp.lt.u32 	%p11, %r25, 100;
	@%p11 bra 	$L__BB15_1;

	ret;

}
	// .globl	_Z24device_write_kernel_flatPd
.visible .entry _Z24device_write_kernel_flatPd(
	.param .u64 _Z24device_write_kernel_flatPd_param_0
)
{
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [_Z24device_write_kernel_flatPd_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r2, %r3, %r1;
	mul.wide.u32 	%rd3, %r4, 8;
	add.s64 	%rd4, %rd2, %rd3;
	mov.u64 	%rd5, 0;
	st.global.u64 	[%rd4], %rd5;
	ret;

}
	// .globl	_Z23device_copy_kernel_flatPdS_
.visible .entry _Z23device_copy_kernel_flatPdS_(
	.param .u64 _Z23device_copy_kernel_flatPdS__param_0,
	.param .u64 _Z23device_copy_kernel_flatPdS__param_1
)
{
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [_Z23device_copy_kernel_flatPdS__param_0];
	ld.param.u64 	%rd2, [_Z23device_copy_kernel_flatPdS__param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mad.lo.s32 	%r4, %r2, %r3, %r1;
	mul.wide.u32 	%rd5, %r4, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f64 	%fd1, [%rd6];
	add.s64 	%rd7, %rd3, %rd5;
	st.global.f64 	[%rd7], %fd1;
	ret;

}
	// .globl	_Z27device_read_kernel_detailedPdmmPl
.visible .entry _Z27device_read_kernel_detailedPdmmPl(
	.param .u64 _Z27device_read_kernel_detailedPdmmPl_param_0,
	.param .u64 _Z27device_read_kernel_detailedPdmmPl_param_1,
	.param .u64 _Z27device_read_kernel_detailedPdmmPl_param_2,
	.param .u64 _Z27device_read_kernel_detailedPdmmPl_param_3
)
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<75>;
	// demoted variable
	.shared .align 8 .b8 _ZZ27device_read_kernel_detailedPdmmPlE12times_vector[8192];

	ld.param.u64 	%rd36, [_Z27device_read_kernel_detailedPdmmPl_param_1];
	ld.param.u64 	%rd37, [_Z27device_read_kernel_detailedPdmmPl_param_2];
	ld.param.u64 	%rd38, [_Z27device_read_kernel_detailedPdmmPl_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mul.lo.s32 	%r3, %r2, %r1;
	mov.u32 	%r4, %tid.x;
	add.s32 	%r9, %r3, %r4;
	cvt.u64.u32 	%rd1, %r9;
	setp.eq.s64 	%p1, %rd37, 0;
	@%p1 bra 	$L__BB18_30;

	cvt.u32.u64 	%r10, %rd1;
	neg.s32 	%r5, %r3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r12, %r2, %r11;
	cvt.u64.u32 	%rd2, %r12;
	shl.b32 	%r13, %r10, 3;
	mov.u32 	%r14, _ZZ27device_read_kernel_detailedPdmmPlE12times_vector;
	add.s32 	%r6, %r14, %r13;
	cvt.u64.u32 	%rd3, %r11;
	cvt.u64.u32 	%rd4, %r1;
	cvt.u64.u32 	%rd40, %r2;
	add.s64 	%rd5, %rd40, -2;
	add.s32 	%r15, %r2, -1;
	and.b32  	%r7, %r15, 3;
	cvt.u64.u32 	%rd41, %r7;
	not.b64 	%rd42, %rd41;
	add.s64 	%rd6, %rd42, %rd40;
	cvta.to.global.u64 	%rd7, %rd38;
	mov.u64 	%rd65, 0;

$L__BB18_2:
	setp.ne.s32 	%p2, %r10, 0;
	@%p2 bra 	$L__BB18_4;

	mov.u64 	%rd43, 0;
	st.shared.u64 	[_ZZ27device_read_kernel_detailedPdmmPlE12times_vector], %rd43;

$L__BB18_4:
	setp.eq.s32 	%p3, %r4, %r5;
	bar.sync 	0;
	@%p3 bra 	$L__BB18_7;
	bra.uni 	$L__BB18_5;

$L__BB18_7:
	// begin inline asm
	mov.u64 	%rd44, %clock64;
	// end inline asm
	add.s64 	%rd66, %rd44, 1979999;
	st.shared.u64 	[_ZZ27device_read_kernel_detailedPdmmPlE12times_vector], %rd66;
	bra.uni 	$L__BB18_8;

$L__BB18_5:
	ld.shared.u64 	%rd66, [_ZZ27device_read_kernel_detailedPdmmPlE12times_vector];
	setp.ne.s64 	%p4, %rd66, 0;
	@%p4 bra 	$L__BB18_8;
	bra.uni 	$L__BB18_6;

$L__BB18_8:
	// begin inline asm
	mov.u64 	%rd45, %clock64;
	// end inline asm
	setp.lt.u64 	%p5, %rd45, %rd66;
	@%p5 bra 	$L__BB18_8;

	setp.ge.u64 	%p6, %rd1, %rd36;
	@%p6 bra 	$L__BB18_19;

	mov.u64 	%rd67, %rd1;

$L__BB18_11:
	.pragma "nounroll";
	add.s64 	%rd13, %rd67, %rd2;
	setp.ge.u64 	%p7, %rd13, %rd36;
	@%p7 bra 	$L__BB18_19;

	add.s64 	%rd14, %rd13, %rd2;
	setp.ge.u64 	%p8, %rd14, %rd36;
	@%p8 bra 	$L__BB18_19;

	add.s64 	%rd15, %rd14, %rd2;
	setp.ge.u64 	%p9, %rd15, %rd36;
	@%p9 bra 	$L__BB18_19;

	add.s64 	%rd16, %rd15, %rd2;
	setp.ge.u64 	%p10, %rd16, %rd36;
	@%p10 bra 	$L__BB18_19;

	add.s64 	%rd17, %rd16, %rd2;
	setp.ge.u64 	%p11, %rd17, %rd36;
	@%p11 bra 	$L__BB18_19;

	add.s64 	%rd18, %rd17, %rd2;
	setp.ge.u64 	%p12, %rd18, %rd36;
	@%p12 bra 	$L__BB18_19;

	add.s64 	%rd19, %rd18, %rd2;
	setp.ge.u64 	%p13, %rd19, %rd36;
	@%p13 bra 	$L__BB18_19;

	add.s64 	%rd67, %rd19, %rd2;
	setp.lt.u64 	%p14, %rd67, %rd36;
	@%p14 bra 	$L__BB18_11;

$L__BB18_19:
	setp.ne.s32 	%p15, %r4, 0;
	// begin inline asm
	mov.u64 	%rd46, %clock64;
	// end inline asm
	sub.s64 	%rd47, %rd46, %rd66;
	st.shared.u64 	[%r6], %rd47;
	bar.sync 	0;
	@%p15 bra 	$L__BB18_29;

	setp.lt.u32 	%p16, %r2, 2;
	ld.shared.u64 	%rd74, [_ZZ27device_read_kernel_detailedPdmmPlE12times_vector];
	@%p16 bra 	$L__BB18_28;

	setp.lt.u64 	%p17, %rd5, 3;
	mov.u64 	%rd72, 1;
	@%p17 bra 	$L__BB18_24;

	mov.u64 	%rd70, %rd6;

$L__BB18_23:
	cvt.u32.u64 	%r17, %rd72;
	shl.b32 	%r18, %r17, 3;
	add.s32 	%r20, %r14, %r18;
	ld.shared.u64 	%rd51, [%r20];
	max.s64 	%rd52, %rd74, %rd51;
	ld.shared.u64 	%rd53, [%r20+8];
	max.s64 	%rd54, %rd52, %rd53;
	ld.shared.u64 	%rd55, [%r20+16];
	max.s64 	%rd56, %rd54, %rd55;
	ld.shared.u64 	%rd57, [%r20+24];
	max.s64 	%rd74, %rd56, %rd57;
	add.s64 	%rd72, %rd72, 4;
	add.s64 	%rd70, %rd70, -4;
	setp.ne.s64 	%p18, %rd70, 0;
	@%p18 bra 	$L__BB18_23;

$L__BB18_24:
	setp.eq.s32 	%p19, %r7, 0;
	@%p19 bra 	$L__BB18_28;

	setp.eq.s32 	%p20, %r7, 1;
	cvt.u32.u64 	%r21, %rd72;
	shl.b32 	%r22, %r21, 3;
	add.s32 	%r8, %r14, %r22;
	ld.shared.u64 	%rd58, [%r8];
	max.s64 	%rd74, %rd74, %rd58;
	@%p20 bra 	$L__BB18_28;

	setp.eq.s32 	%p21, %r7, 2;
	ld.shared.u64 	%rd59, [%r8+8];
	max.s64 	%rd74, %rd74, %rd59;
	@%p21 bra 	$L__BB18_28;

	ld.shared.u64 	%rd60, [%r8+16];
	max.s64 	%rd74, %rd74, %rd60;

$L__BB18_28:
	mul.lo.s64 	%rd61, %rd65, %rd3;
	add.s64 	%rd62, %rd61, %rd4;
	shl.b64 	%rd63, %rd62, 3;
	add.s64 	%rd64, %rd7, %rd63;
	st.global.u64 	[%rd64], %rd74;

$L__BB18_29:
	add.s64 	%rd65, %rd65, 1;
	setp.lt.u64 	%p22, %rd65, %rd37;
	@%p22 bra 	$L__BB18_2;

$L__BB18_30:
	ret;

$L__BB18_6:
	bra.uni 	$L__BB18_6;

}
	// .globl	_Z18device_read_kernelPdm
.visible .entry _Z18device_read_kernelPdm(
	.param .u64 _Z18device_read_kernelPdm_param_0,
	.param .u64 _Z18device_read_kernelPdm_param_1
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd12, [_Z18device_read_kernelPdm_param_1];
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mad.lo.s32 	%r4, %r1, %r3, %r2;
	cvt.u64.u32 	%rd13, %r4;
	setp.ge.u64 	%p1, %rd13, %rd12;
	@%p1 bra 	$L__BB19_10;

	mov.u32 	%r5, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r5;
	cvt.u64.u32 	%rd2, %r6;

$L__BB19_2:
	.pragma "nounroll";
	add.s64 	%rd4, %rd13, %rd2;
	setp.ge.u64 	%p2, %rd4, %rd12;
	@%p2 bra 	$L__BB19_10;

	add.s64 	%rd5, %rd4, %rd2;
	setp.ge.u64 	%p3, %rd5, %rd12;
	@%p3 bra 	$L__BB19_10;

	add.s64 	%rd6, %rd5, %rd2;
	setp.ge.u64 	%p4, %rd6, %rd12;
	@%p4 bra 	$L__BB19_10;

	add.s64 	%rd7, %rd6, %rd2;
	setp.ge.u64 	%p5, %rd7, %rd12;
	@%p5 bra 	$L__BB19_10;

	add.s64 	%rd8, %rd7, %rd2;
	setp.ge.u64 	%p6, %rd8, %rd12;
	@%p6 bra 	$L__BB19_10;

	add.s64 	%rd9, %rd8, %rd2;
	setp.ge.u64 	%p7, %rd9, %rd12;
	@%p7 bra 	$L__BB19_10;

	add.s64 	%rd10, %rd9, %rd2;
	setp.ge.u64 	%p8, %rd10, %rd12;
	@%p8 bra 	$L__BB19_10;

	add.s64 	%rd13, %rd10, %rd2;
	setp.lt.u64 	%p9, %rd13, %rd12;
	@%p9 bra 	$L__BB19_2;

$L__BB19_10:
	ret;

}
	// .globl	_Z19device_write_kernelPdm
.visible .entry _Z19device_write_kernelPdm(
	.param .u64 _Z19device_write_kernelPdm_param_0,
	.param .u64 _Z19device_write_kernelPdm_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<7>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd6, [_Z19device_write_kernelPdm_param_0];
	ld.param.u64 	%rd7, [_Z19device_write_kernelPdm_param_1];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd11, %r4;
	setp.ge.u64 	%p1, %rd11, %rd7;
	@%p1 bra 	$L__BB20_3;

	mov.u32 	%r5, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r5;
	cvt.u64.u32 	%rd2, %r6;
	cvta.to.global.u64 	%rd3, %rd6;

$L__BB20_2:
	shl.b64 	%rd8, %rd11, 3;
	add.s64 	%rd9, %rd3, %rd8;
	mov.u64 	%rd10, 0;
	st.global.u64 	[%rd9], %rd10;
	add.s64 	%rd11, %rd11, %rd2;
	setp.lt.u64 	%p2, %rd11, %rd7;
	@%p2 bra 	$L__BB20_2;

$L__BB20_3:
	ret;

}
	// .globl	_Z18device_copy_kernelPdS_m
.visible .entry _Z18device_copy_kernelPdS_m(
	.param .u64 _Z18device_copy_kernelPdS_m_param_0,
	.param .u64 _Z18device_copy_kernelPdS_m_param_1,
	.param .u64 _Z18device_copy_kernelPdS_m_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd7, [_Z18device_copy_kernelPdS_m_param_0];
	ld.param.u64 	%rd8, [_Z18device_copy_kernelPdS_m_param_1];
	ld.param.u64 	%rd9, [_Z18device_copy_kernelPdS_m_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd13, %r4;
	setp.ge.u64 	%p1, %rd13, %rd9;
	@%p1 bra 	$L__BB21_3;

	mov.u32 	%r5, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r5;
	cvt.u64.u32 	%rd2, %r6;
	cvta.to.global.u64 	%rd3, %rd7;
	cvta.to.global.u64 	%rd4, %rd8;

$L__BB21_2:
	shl.b64 	%rd10, %rd13, 3;
	add.s64 	%rd11, %rd3, %rd10;
	ld.global.f64 	%fd1, [%rd11];
	add.s64 	%rd12, %rd4, %rd10;
	st.global.f64 	[%rd12], %fd1;
	add.s64 	%rd13, %rd13, %rd2;
	setp.lt.u64 	%p2, %rd13, %rd9;
	@%p2 bra 	$L__BB21_2;

$L__BB21_3:
	ret;

}
	// .globl	_Z23device_read_kernel_syncPdmmPlS0_
.visible .entry _Z23device_read_kernel_syncPdmmPlS0_(
	.param .u64 _Z23device_read_kernel_syncPdmmPlS0__param_0,
	.param .u64 _Z23device_read_kernel_syncPdmmPlS0__param_1,
	.param .u64 _Z23device_read_kernel_syncPdmmPlS0__param_2,
	.param .u64 _Z23device_read_kernel_syncPdmmPlS0__param_3,
	.param .u64 _Z23device_read_kernel_syncPdmmPlS0__param_4
)
{
	.local .align 8 .b8 	__local_depot22[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<31>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<133>;
	// demoted variable
	.shared .align 8 .b8 _ZZ23device_read_kernel_syncPdmmPlS0_E6clocks[8192];

	mov.u64 	%SPL, __local_depot22;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd57, [_Z23device_read_kernel_syncPdmmPlS0__param_0];
	ld.param.u64 	%rd58, [_Z23device_read_kernel_syncPdmmPlS0__param_1];
	ld.param.u64 	%rd59, [_Z23device_read_kernel_syncPdmmPlS0__param_2];
	ld.param.u64 	%rd60, [_Z23device_read_kernel_syncPdmmPlS0__param_3];
	ld.param.u64 	%rd62, [_Z23device_read_kernel_syncPdmmPlS0__param_4];
	cvta.to.global.u64 	%rd1, %rd62;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r15, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r15;
	// begin inline asm
	mov.u32 %r13, %envreg2;
	// end inline asm
	cvt.u64.u32 	%rd63, %r13;
	// begin inline asm
	mov.u32 %r14, %envreg1;
	// end inline asm
	cvt.u64.u32 	%rd64, %r14;
	bfi.b64 	%rd3, %rd64, %rd63, 32, 32;
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB22_40;

	add.s64 	%rd4, %rd3, 4;
	mov.u32 	%r16, %tid.y;
	add.s32 	%r4, %r3, %r16;
	mov.u32 	%r17, %tid.z;
	neg.s32 	%r5, %r17;
	mov.u32 	%r18, %nctaid.y;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r20, %r19, %r18;
	mov.u32 	%r21, %nctaid.z;
	mul.lo.s32 	%r22, %r20, %r21;
	mov.u32 	%r23, %ctaid.y;
	add.s32 	%r24, %r1, %r23;
	mov.u32 	%r25, %ctaid.z;
	neg.s32 	%r26, %r25;
	setp.eq.s32 	%p2, %r24, %r26;
	mov.u32 	%r27, -2147483647;
	sub.s32 	%r28, %r27, %r22;
	selp.b32 	%r6, %r28, 1, %p2;
	mul.lo.s32 	%r29, %r2, %r19;
	cvt.u64.u32 	%rd5, %r29;
	shl.b32 	%r30, %r3, 3;
	mov.u32 	%r31, _ZZ23device_read_kernel_syncPdmmPlS0_E6clocks;
	add.s32 	%r7, %r31, %r30;
	cvt.u64.u32 	%rd6, %r19;
	cvt.u64.u32 	%rd7, %r1;
	cvt.u64.u32 	%rd67, %r2;
	add.s64 	%rd8, %rd67, -2;
	add.s32 	%r32, %r2, -1;
	and.b32  	%r8, %r32, 3;
	cvt.u64.u32 	%rd68, %r8;
	not.b64 	%rd69, %rd68;
	add.s64 	%rd9, %rd69, %rd67;
	mul.wide.u32 	%rd10, %r29, 8;
	cvta.to.global.u64 	%rd11, %rd60;
	mov.u64 	%rd120, 0;

$L__BB22_2:
	setp.ne.s64 	%p3, %rd3, 0;
	@%p3 bra 	$L__BB22_4;

	// begin inline asm
	trap;
	// end inline asm

$L__BB22_4:
	setp.ne.s32 	%p4, %r4, %r5;
	barrier.sync 	0;
	@%p4 bra 	$L__BB22_7;

	// begin inline asm
	atom.add.release.gpu.u32 %r33,[%rd4],%r6;
	// end inline asm

$L__BB22_6:
	// begin inline asm
	ld.acquire.gpu.u32 %r35,[%rd4];
	// end inline asm
	xor.b32  	%r36, %r35, %r33;
	setp.gt.s32 	%p5, %r36, -1;
	@%p5 bra 	$L__BB22_6;

$L__BB22_7:
	barrier.sync 	0;
	// begin inline asm
	mov.u64 %rd72, %globaltimer;
	// end inline asm
	add.s64 	%rd73, %rd72, 1000000;
	st.global.u64 	[%rd1], %rd73;
	@%p3 bra 	$L__BB22_9;

	// begin inline asm
	trap;
	// end inline asm

$L__BB22_9:
	barrier.sync 	0;
	@%p4 bra 	$L__BB22_12;

	// begin inline asm
	atom.add.release.gpu.u32 %r37,[%rd4],%r6;
	// end inline asm

$L__BB22_11:
	// begin inline asm
	ld.acquire.gpu.u32 %r39,[%rd4];
	// end inline asm
	xor.b32  	%r40, %r39, %r37;
	setp.gt.s32 	%p8, %r40, -1;
	@%p8 bra 	$L__BB22_11;

$L__BB22_12:
	barrier.sync 	0;
	bar.sync 	0;
	ld.global.u64 	%rd14, [%rd1];

$L__BB22_13:
	// begin inline asm
	mov.u64 %rd76, %globaltimer;
	// end inline asm
	setp.lt.s64 	%p9, %rd76, %rd14;
	@%p9 bra 	$L__BB22_13;

	setp.ge.u64 	%p10, %rd2, %rd58;
	@%p10 bra 	$L__BB22_24;

	mov.u64 	%rd122, %rd2;

$L__BB22_16:
	.pragma "nounroll";
	shl.b64 	%rd79, %rd122, 3;
	add.s64 	%rd78, %rd57, %rd79;
	// begin inline asm
	ld.ca.u64 %rd77, [%rd78];
	// end inline asm
	xor.b64  	%rd124, %rd77, %rd124;
	add.s64 	%rd19, %rd122, %rd5;
	setp.ge.u64 	%p11, %rd19, %rd58;
	@%p11 bra 	$L__BB22_24;

	add.s64 	%rd81, %rd78, %rd10;
	// begin inline asm
	ld.ca.u64 %rd80, [%rd81];
	// end inline asm
	xor.b64  	%rd124, %rd80, %rd124;
	add.s64 	%rd22, %rd19, %rd5;
	setp.ge.u64 	%p12, %rd22, %rd58;
	@%p12 bra 	$L__BB22_24;

	add.s64 	%rd83, %rd81, %rd10;
	// begin inline asm
	ld.ca.u64 %rd82, [%rd83];
	// end inline asm
	xor.b64  	%rd124, %rd82, %rd124;
	add.s64 	%rd25, %rd22, %rd5;
	setp.ge.u64 	%p13, %rd25, %rd58;
	@%p13 bra 	$L__BB22_24;

	add.s64 	%rd85, %rd83, %rd10;
	// begin inline asm
	ld.ca.u64 %rd84, [%rd85];
	// end inline asm
	xor.b64  	%rd124, %rd84, %rd124;
	add.s64 	%rd28, %rd25, %rd5;
	setp.ge.u64 	%p14, %rd28, %rd58;
	@%p14 bra 	$L__BB22_24;

	add.s64 	%rd87, %rd85, %rd10;
	// begin inline asm
	ld.ca.u64 %rd86, [%rd87];
	// end inline asm
	xor.b64  	%rd124, %rd86, %rd124;
	add.s64 	%rd31, %rd28, %rd5;
	setp.ge.u64 	%p15, %rd31, %rd58;
	@%p15 bra 	$L__BB22_24;

	add.s64 	%rd89, %rd87, %rd10;
	// begin inline asm
	ld.ca.u64 %rd88, [%rd89];
	// end inline asm
	xor.b64  	%rd124, %rd88, %rd124;
	add.s64 	%rd34, %rd31, %rd5;
	setp.ge.u64 	%p16, %rd34, %rd58;
	@%p16 bra 	$L__BB22_24;

	add.s64 	%rd91, %rd89, %rd10;
	// begin inline asm
	ld.ca.u64 %rd90, [%rd91];
	// end inline asm
	xor.b64  	%rd124, %rd90, %rd124;
	add.s64 	%rd37, %rd34, %rd5;
	setp.ge.u64 	%p17, %rd37, %rd58;
	@%p17 bra 	$L__BB22_24;

	add.s64 	%rd93, %rd91, %rd10;
	// begin inline asm
	ld.ca.u64 %rd92, [%rd93];
	// end inline asm
	xor.b64  	%rd124, %rd92, %rd124;
	add.s64 	%rd122, %rd37, %rd5;
	setp.lt.u64 	%p18, %rd122, %rd58;
	@%p18 bra 	$L__BB22_16;

$L__BB22_24:
	// begin inline asm
	mov.u64 %rd94, %globaltimer;
	// end inline asm
	sub.s64 	%rd95, %rd94, %rd14;
	st.shared.u64 	[%r7], %rd95;
	@%p3 bra 	$L__BB22_26;

	// begin inline asm
	trap;
	// end inline asm

$L__BB22_26:
	barrier.sync 	0;
	@%p4 bra 	$L__BB22_29;

	// begin inline asm
	atom.add.release.gpu.u32 %r41,[%rd4],%r6;
	// end inline asm

$L__BB22_28:
	// begin inline asm
	ld.acquire.gpu.u32 %r43,[%rd4];
	// end inline asm
	xor.b32  	%r44, %r43, %r41;
	setp.gt.s32 	%p21, %r44, -1;
	@%p21 bra 	$L__BB22_28;

$L__BB22_29:
	setp.ne.s32 	%p22, %r3, 0;
	barrier.sync 	0;
	@%p22 bra 	$L__BB22_39;

	setp.lt.u32 	%p23, %r2, 2;
	ld.shared.u64 	%rd131, [_ZZ23device_read_kernel_syncPdmmPlS0_E6clocks];
	@%p23 bra 	$L__BB22_38;

	setp.lt.u64 	%p24, %rd8, 3;
	mov.u64 	%rd129, 1;
	@%p24 bra 	$L__BB22_34;

	mov.u64 	%rd127, %rd9;

$L__BB22_33:
	cvt.u32.u64 	%r45, %rd129;
	shl.b32 	%r46, %r45, 3;
	add.s32 	%r48, %r31, %r46;
	ld.shared.u64 	%rd101, [%r48];
	max.s64 	%rd102, %rd131, %rd101;
	ld.shared.u64 	%rd103, [%r48+8];
	max.s64 	%rd104, %rd102, %rd103;
	ld.shared.u64 	%rd105, [%r48+16];
	max.s64 	%rd106, %rd104, %rd105;
	ld.shared.u64 	%rd107, [%r48+24];
	max.s64 	%rd131, %rd106, %rd107;
	add.s64 	%rd129, %rd129, 4;
	add.s64 	%rd127, %rd127, -4;
	setp.ne.s64 	%p25, %rd127, 0;
	@%p25 bra 	$L__BB22_33;

$L__BB22_34:
	setp.eq.s32 	%p26, %r8, 0;
	@%p26 bra 	$L__BB22_38;

	setp.eq.s32 	%p27, %r8, 1;
	cvt.u32.u64 	%r49, %rd129;
	shl.b32 	%r50, %r49, 3;
	add.s32 	%r12, %r31, %r50;
	ld.shared.u64 	%rd108, [%r12];
	max.s64 	%rd131, %rd131, %rd108;
	@%p27 bra 	$L__BB22_38;

	setp.eq.s32 	%p28, %r8, 2;
	ld.shared.u64 	%rd109, [%r12+8];
	max.s64 	%rd131, %rd131, %rd109;
	@%p28 bra 	$L__BB22_38;

	ld.shared.u64 	%rd110, [%r12+16];
	max.s64 	%rd131, %rd131, %rd110;

$L__BB22_38:
	mul.lo.s64 	%rd111, %rd120, %rd6;
	add.s64 	%rd112, %rd111, %rd7;
	shl.b64 	%rd113, %rd112, 3;
	add.s64 	%rd114, %rd11, %rd113;
	st.global.u64 	[%rd114], %rd131;

$L__BB22_39:
	add.s64 	%rd120, %rd120, 1;
	setp.lt.u64 	%p29, %rd120, %rd59;
	@%p29 bra 	$L__BB22_2;

$L__BB22_40:
	ld.global.u64 	%rd115, [%rd1];
	setp.ne.s64 	%p30, %rd115, 0;
	@%p30 bra 	$L__BB22_42;

	add.u64 	%rd116, %SP, 0;
	add.u64 	%rd117, %SPL, 0;
	st.local.u64 	[%rd117], %rd124;
	mov.u64 	%rd118, $str$2;
	cvta.global.u64 	%rd119, %rd118;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd119;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd116;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r52, [retval0+0];
	} // callseq 1

$L__BB22_42:
	ret;

}
	// .globl	_Z24device_write_kernel_syncPdmmPlS0_
.visible .entry _Z24device_write_kernel_syncPdmmPlS0_(
	.param .u64 _Z24device_write_kernel_syncPdmmPlS0__param_0,
	.param .u64 _Z24device_write_kernel_syncPdmmPlS0__param_1,
	.param .u64 _Z24device_write_kernel_syncPdmmPlS0__param_2,
	.param .u64 _Z24device_write_kernel_syncPdmmPlS0__param_3,
	.param .u64 _Z24device_write_kernel_syncPdmmPlS0__param_4
)
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<82>;
	// demoted variable
	.shared .align 8 .b8 _ZZ24device_write_kernel_syncPdmmPlS0_E6clocks[8192];

	ld.param.u64 	%rd31, [_Z24device_write_kernel_syncPdmmPlS0__param_0];
	ld.param.u64 	%rd32, [_Z24device_write_kernel_syncPdmmPlS0__param_1];
	ld.param.u64 	%rd33, [_Z24device_write_kernel_syncPdmmPlS0__param_2];
	ld.param.u64 	%rd34, [_Z24device_write_kernel_syncPdmmPlS0__param_3];
	ld.param.u64 	%rd35, [_Z24device_write_kernel_syncPdmmPlS0__param_4];
	cvta.to.global.u64 	%rd1, %rd35;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r15, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r15;
	// begin inline asm
	mov.u32 %r13, %envreg2;
	// end inline asm
	cvt.u64.u32 	%rd36, %r13;
	// begin inline asm
	mov.u32 %r14, %envreg1;
	// end inline asm
	cvt.u64.u32 	%rd37, %r14;
	bfi.b64 	%rd3, %rd37, %rd36, 32, 32;
	setp.eq.s64 	%p1, %rd33, 0;
	@%p1 bra 	$L__BB23_33;

	add.s64 	%rd4, %rd3, 4;
	mov.u32 	%r16, %tid.y;
	add.s32 	%r4, %r3, %r16;
	mov.u32 	%r17, %tid.z;
	neg.s32 	%r5, %r17;
	mov.u32 	%r18, %nctaid.y;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r20, %r19, %r18;
	mov.u32 	%r21, %nctaid.z;
	mul.lo.s32 	%r22, %r20, %r21;
	mov.u32 	%r23, %ctaid.y;
	add.s32 	%r24, %r2, %r23;
	mov.u32 	%r25, %ctaid.z;
	neg.s32 	%r26, %r25;
	setp.eq.s32 	%p2, %r24, %r26;
	mov.u32 	%r27, -2147483647;
	sub.s32 	%r28, %r27, %r22;
	selp.b32 	%r6, %r28, 1, %p2;
	shl.b32 	%r29, %r3, 3;
	mov.u32 	%r30, _ZZ24device_write_kernel_syncPdmmPlS0_E6clocks;
	add.s32 	%r7, %r30, %r29;
	mul.lo.s32 	%r31, %r1, %r19;
	cvt.u64.u32 	%rd5, %r31;
	cvt.u64.u32 	%rd6, %r19;
	cvt.u64.u32 	%rd7, %r2;
	cvt.u64.u32 	%rd39, %r1;
	add.s64 	%rd8, %rd39, -2;
	add.s32 	%r32, %r1, -1;
	and.b32  	%r8, %r32, 3;
	cvt.u64.u32 	%rd40, %r8;
	not.b64 	%rd41, %rd40;
	add.s64 	%rd9, %rd41, %rd39;
	cvta.to.global.u64 	%rd10, %rd34;
	cvta.to.global.u64 	%rd11, %rd31;
	mov.u64 	%rd73, 0;

$L__BB23_2:
	setp.ne.s64 	%p3, %rd3, 0;
	@%p3 bra 	$L__BB23_4;

	// begin inline asm
	trap;
	// end inline asm

$L__BB23_4:
	setp.ne.s32 	%p4, %r4, %r5;
	barrier.sync 	0;
	@%p4 bra 	$L__BB23_7;

	// begin inline asm
	atom.add.release.gpu.u32 %r33,[%rd4],%r6;
	// end inline asm

$L__BB23_6:
	// begin inline asm
	ld.acquire.gpu.u32 %r35,[%rd4];
	// end inline asm
	xor.b32  	%r36, %r35, %r33;
	setp.gt.s32 	%p5, %r36, -1;
	@%p5 bra 	$L__BB23_6;

$L__BB23_7:
	barrier.sync 	0;
	// begin inline asm
	mov.u64 %rd44, %globaltimer;
	// end inline asm
	add.s64 	%rd45, %rd44, 1000000;
	st.global.u64 	[%rd1], %rd45;
	@%p3 bra 	$L__BB23_9;

	// begin inline asm
	trap;
	// end inline asm

$L__BB23_9:
	barrier.sync 	0;
	@%p4 bra 	$L__BB23_12;

	// begin inline asm
	atom.add.release.gpu.u32 %r37,[%rd4],%r6;
	// end inline asm

$L__BB23_11:
	// begin inline asm
	ld.acquire.gpu.u32 %r39,[%rd4];
	// end inline asm
	xor.b32  	%r40, %r39, %r37;
	setp.gt.s32 	%p8, %r40, -1;
	@%p8 bra 	$L__BB23_11;

$L__BB23_12:
	barrier.sync 	0;
	bar.sync 	0;
	ld.global.u64 	%rd13, [%rd1];

$L__BB23_13:
	// begin inline asm
	mov.u64 %rd48, %globaltimer;
	// end inline asm
	setp.lt.s64 	%p9, %rd48, %rd13;
	@%p9 bra 	$L__BB23_13;

	setp.ge.u64 	%p10, %rd2, %rd32;
	@%p10 bra 	$L__BB23_17;

	mov.u64 	%rd74, %rd2;

$L__BB23_16:
	shl.b64 	%rd49, %rd74, 3;
	add.s64 	%rd50, %rd11, %rd49;
	mov.u64 	%rd51, 0;
	st.global.u64 	[%rd50], %rd51;
	add.s64 	%rd74, %rd74, %rd5;
	setp.lt.u64 	%p11, %rd74, %rd32;
	@%p11 bra 	$L__BB23_16;

$L__BB23_17:
	// begin inline asm
	mov.u64 %rd52, %globaltimer;
	// end inline asm
	sub.s64 	%rd53, %rd52, %rd13;
	st.shared.u64 	[%r7], %rd53;
	@%p3 bra 	$L__BB23_19;

	// begin inline asm
	trap;
	// end inline asm

$L__BB23_19:
	barrier.sync 	0;
	@%p4 bra 	$L__BB23_22;

	// begin inline asm
	atom.add.release.gpu.u32 %r41,[%rd4],%r6;
	// end inline asm

$L__BB23_21:
	// begin inline asm
	ld.acquire.gpu.u32 %r43,[%rd4];
	// end inline asm
	xor.b32  	%r44, %r43, %r41;
	setp.gt.s32 	%p14, %r44, -1;
	@%p14 bra 	$L__BB23_21;

$L__BB23_22:
	setp.ne.s32 	%p15, %r3, 0;
	barrier.sync 	0;
	@%p15 bra 	$L__BB23_32;

	setp.lt.u32 	%p16, %r1, 2;
	ld.shared.u64 	%rd81, [_ZZ24device_write_kernel_syncPdmmPlS0_E6clocks];
	@%p16 bra 	$L__BB23_31;

	setp.lt.u64 	%p17, %rd8, 3;
	mov.u64 	%rd79, 1;
	@%p17 bra 	$L__BB23_27;

	mov.u64 	%rd77, %rd9;

$L__BB23_26:
	cvt.u32.u64 	%r45, %rd79;
	shl.b32 	%r46, %r45, 3;
	add.s32 	%r48, %r30, %r46;
	ld.shared.u64 	%rd59, [%r48];
	max.s64 	%rd60, %rd81, %rd59;
	ld.shared.u64 	%rd61, [%r48+8];
	max.s64 	%rd62, %rd60, %rd61;
	ld.shared.u64 	%rd63, [%r48+16];
	max.s64 	%rd64, %rd62, %rd63;
	ld.shared.u64 	%rd65, [%r48+24];
	max.s64 	%rd81, %rd64, %rd65;
	add.s64 	%rd79, %rd79, 4;
	add.s64 	%rd77, %rd77, -4;
	setp.ne.s64 	%p18, %rd77, 0;
	@%p18 bra 	$L__BB23_26;

$L__BB23_27:
	setp.eq.s32 	%p19, %r8, 0;
	@%p19 bra 	$L__BB23_31;

	setp.eq.s32 	%p20, %r8, 1;
	cvt.u32.u64 	%r49, %rd79;
	shl.b32 	%r50, %r49, 3;
	add.s32 	%r12, %r30, %r50;
	ld.shared.u64 	%rd66, [%r12];
	max.s64 	%rd81, %rd81, %rd66;
	@%p20 bra 	$L__BB23_31;

	setp.eq.s32 	%p21, %r8, 2;
	ld.shared.u64 	%rd67, [%r12+8];
	max.s64 	%rd81, %rd81, %rd67;
	@%p21 bra 	$L__BB23_31;

	ld.shared.u64 	%rd68, [%r12+16];
	max.s64 	%rd81, %rd81, %rd68;

$L__BB23_31:
	mul.lo.s64 	%rd69, %rd73, %rd6;
	add.s64 	%rd70, %rd69, %rd7;
	shl.b64 	%rd71, %rd70, 3;
	add.s64 	%rd72, %rd10, %rd71;
	st.global.u64 	[%rd72], %rd81;

$L__BB23_32:
	add.s64 	%rd73, %rd73, 1;
	setp.lt.u64 	%p22, %rd73, %rd33;
	@%p22 bra 	$L__BB23_2;

$L__BB23_33:
	ret;

}
	// .globl	_Z33device_write_kernel_time_analysisPdmmPlS0_S0_
.visible .entry _Z33device_write_kernel_time_analysisPdmmPlS0_S0_(
	.param .u64 _Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_0,
	.param .u64 _Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_1,
	.param .u64 _Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_2,
	.param .u64 _Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_3,
	.param .u64 _Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_4,
	.param .u64 _Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_5
)
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<48>;


	ld.param.u64 	%rd17, [_Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_0];
	ld.param.u64 	%rd18, [_Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_1];
	ld.param.u64 	%rd19, [_Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_2];
	ld.param.u64 	%rd20, [_Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_3];
	ld.param.u64 	%rd21, [_Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_4];
	ld.param.u64 	%rd22, [_Z33device_write_kernel_time_analysisPdmmPlS0_S0__param_5];
	cvta.to.global.u64 	%rd1, %rd22;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r12, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r12;
	// begin inline asm
	mov.u32 %r10, %envreg2;
	// end inline asm
	cvt.u64.u32 	%rd23, %r10;
	// begin inline asm
	mov.u32 %r11, %envreg1;
	// end inline asm
	cvt.u64.u32 	%rd24, %r11;
	bfi.b64 	%rd3, %rd24, %rd23, 32, 32;
	setp.eq.s64 	%p1, %rd19, 0;
	@%p1 bra 	$L__BB24_23;

	add.s64 	%rd4, %rd3, 4;
	mov.u32 	%r13, %tid.y;
	add.s32 	%r4, %r3, %r13;
	mov.u32 	%r14, %tid.z;
	neg.s32 	%r5, %r14;
	mov.u32 	%r15, %nctaid.y;
	mov.u32 	%r16, %nctaid.x;
	mul.lo.s32 	%r17, %r16, %r15;
	mov.u32 	%r18, %nctaid.z;
	mul.lo.s32 	%r19, %r17, %r18;
	mov.u32 	%r20, %ctaid.y;
	add.s32 	%r21, %r2, %r20;
	mov.u32 	%r22, %ctaid.z;
	neg.s32 	%r23, %r22;
	setp.eq.s32 	%p2, %r21, %r23;
	mov.u32 	%r24, -2147483647;
	sub.s32 	%r25, %r24, %r19;
	selp.b32 	%r6, %r25, 1, %p2;
	mul.lo.s32 	%r26, %r1, %r16;
	cvt.u64.u32 	%rd5, %r26;
	mul.wide.u32 	%rd6, %r1, %r16;
	cvta.to.global.u64 	%rd7, %rd20;
	cvta.to.global.u64 	%rd8, %rd21;
	cvta.to.global.u64 	%rd9, %rd17;
	mov.u64 	%rd46, 0;

$L__BB24_2:
	setp.ne.s64 	%p3, %rd3, 0;
	@%p3 bra 	$L__BB24_4;

	// begin inline asm
	trap;
	// end inline asm

$L__BB24_4:
	setp.ne.s32 	%p4, %r4, %r5;
	barrier.sync 	0;
	@%p4 bra 	$L__BB24_7;

	// begin inline asm
	atom.add.release.gpu.u32 %r27,[%rd4],%r6;
	// end inline asm

$L__BB24_6:
	// begin inline asm
	ld.acquire.gpu.u32 %r29,[%rd4];
	// end inline asm
	xor.b32  	%r30, %r29, %r27;
	setp.gt.s32 	%p5, %r30, -1;
	@%p5 bra 	$L__BB24_6;

$L__BB24_7:
	barrier.sync 	0;
	// begin inline asm
	mov.u64 %rd28, %globaltimer;
	// end inline asm
	add.s64 	%rd29, %rd28, 1000000;
	st.global.u64 	[%rd1], %rd29;
	@%p3 bra 	$L__BB24_9;

	// begin inline asm
	trap;
	// end inline asm

$L__BB24_9:
	barrier.sync 	0;
	@%p4 bra 	$L__BB24_12;

	// begin inline asm
	atom.add.release.gpu.u32 %r31,[%rd4],%r6;
	// end inline asm

$L__BB24_11:
	// begin inline asm
	ld.acquire.gpu.u32 %r33,[%rd4];
	// end inline asm
	xor.b32  	%r34, %r33, %r31;
	setp.gt.s32 	%p8, %r34, -1;
	@%p8 bra 	$L__BB24_11;

$L__BB24_12:
	barrier.sync 	0;
	bar.sync 	0;
	ld.global.u64 	%rd11, [%rd1];

$L__BB24_13:
	// begin inline asm
	mov.u64 %rd32, %globaltimer;
	// end inline asm
	setp.lt.s64 	%p9, %rd32, %rd11;
	@%p9 bra 	$L__BB24_13;

	setp.ge.u64 	%p10, %rd2, %rd18;
	@%p10 bra 	$L__BB24_17;

	mov.u64 	%rd47, %rd2;

$L__BB24_16:
	shl.b64 	%rd33, %rd47, 3;
	add.s64 	%rd34, %rd9, %rd33;
	mov.u64 	%rd35, 0;
	st.global.u64 	[%rd34], %rd35;
	add.s64 	%rd47, %rd47, %rd5;
	setp.lt.u64 	%p11, %rd47, %rd18;
	@%p11 bra 	$L__BB24_16;

$L__BB24_17:
	// begin inline asm
	mov.u64 %rd36, %globaltimer;
	// end inline asm
	@%p3 bra 	$L__BB24_19;

	// begin inline asm
	trap;
	// end inline asm

$L__BB24_19:
	barrier.sync 	0;
	@%p4 bra 	$L__BB24_22;

	// begin inline asm
	atom.add.release.gpu.u32 %r35,[%rd4],%r6;
	// end inline asm

$L__BB24_21:
	// begin inline asm
	ld.acquire.gpu.u32 %r37,[%rd4];
	// end inline asm
	xor.b32  	%r38, %r37, %r35;
	setp.gt.s32 	%p14, %r38, -1;
	@%p14 bra 	$L__BB24_21;

$L__BB24_22:
	barrier.sync 	0;
	mul.lo.s64 	%rd39, %rd6, %rd46;
	add.s64 	%rd40, %rd39, %rd2;
	shl.b64 	%rd41, %rd40, 3;
	add.s64 	%rd42, %rd7, %rd41;
	sub.s64 	%rd43, %rd32, %rd11;
	st.global.u64 	[%rd42], %rd43;
	add.s64 	%rd44, %rd8, %rd41;
	sub.s64 	%rd45, %rd36, %rd11;
	st.global.u64 	[%rd44], %rd45;
	add.s64 	%rd46, %rd46, 1;
	setp.lt.u64 	%p15, %rd46, %rd19;
	@%p15 bra 	$L__BB24_2;

$L__BB24_23:
	ret;

}
	// .globl	_Z23device_copy_kernel_syncPdS_mmPlS0_
.visible .entry _Z23device_copy_kernel_syncPdS_mmPlS0_(
	.param .u64 _Z23device_copy_kernel_syncPdS_mmPlS0__param_0,
	.param .u64 _Z23device_copy_kernel_syncPdS_mmPlS0__param_1,
	.param .u64 _Z23device_copy_kernel_syncPdS_mmPlS0__param_2,
	.param .u64 _Z23device_copy_kernel_syncPdS_mmPlS0__param_3,
	.param .u64 _Z23device_copy_kernel_syncPdS_mmPlS0__param_4,
	.param .u64 _Z23device_copy_kernel_syncPdS_mmPlS0__param_5
)
{
	.reg .pred 	%p<23>;
	.reg .b32 	%r<52>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<84>;
	// demoted variable
	.shared .align 8 .b8 _ZZ23device_copy_kernel_syncPdS_mmPlS0_E6clocks[8192];

	ld.param.u64 	%rd32, [_Z23device_copy_kernel_syncPdS_mmPlS0__param_0];
	ld.param.u64 	%rd33, [_Z23device_copy_kernel_syncPdS_mmPlS0__param_1];
	ld.param.u64 	%rd34, [_Z23device_copy_kernel_syncPdS_mmPlS0__param_2];
	ld.param.u64 	%rd35, [_Z23device_copy_kernel_syncPdS_mmPlS0__param_3];
	ld.param.u64 	%rd36, [_Z23device_copy_kernel_syncPdS_mmPlS0__param_4];
	ld.param.u64 	%rd37, [_Z23device_copy_kernel_syncPdS_mmPlS0__param_5];
	cvta.to.global.u64 	%rd1, %rd37;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r15, %r2, %r1, %r3;
	cvt.u64.u32 	%rd2, %r15;
	// begin inline asm
	mov.u32 %r13, %envreg2;
	// end inline asm
	cvt.u64.u32 	%rd38, %r13;
	// begin inline asm
	mov.u32 %r14, %envreg1;
	// end inline asm
	cvt.u64.u32 	%rd39, %r14;
	bfi.b64 	%rd3, %rd39, %rd38, 32, 32;
	setp.eq.s64 	%p1, %rd35, 0;
	@%p1 bra 	$L__BB25_33;

	add.s64 	%rd4, %rd3, 4;
	mov.u32 	%r16, %tid.y;
	add.s32 	%r4, %r3, %r16;
	mov.u32 	%r17, %tid.z;
	neg.s32 	%r5, %r17;
	mov.u32 	%r18, %nctaid.y;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r20, %r19, %r18;
	mov.u32 	%r21, %nctaid.z;
	mul.lo.s32 	%r22, %r20, %r21;
	mov.u32 	%r23, %ctaid.y;
	add.s32 	%r24, %r2, %r23;
	mov.u32 	%r25, %ctaid.z;
	neg.s32 	%r26, %r25;
	setp.eq.s32 	%p2, %r24, %r26;
	mov.u32 	%r27, -2147483647;
	sub.s32 	%r28, %r27, %r22;
	selp.b32 	%r6, %r28, 1, %p2;
	shl.b32 	%r29, %r3, 3;
	mov.u32 	%r30, _ZZ23device_copy_kernel_syncPdS_mmPlS0_E6clocks;
	add.s32 	%r7, %r30, %r29;
	mul.lo.s32 	%r31, %r1, %r19;
	cvt.u64.u32 	%rd5, %r31;
	cvt.u64.u32 	%rd6, %r19;
	cvt.u64.u32 	%rd7, %r2;
	cvt.u64.u32 	%rd41, %r1;
	add.s64 	%rd8, %rd41, -2;
	add.s32 	%r32, %r1, -1;
	and.b32  	%r8, %r32, 3;
	cvt.u64.u32 	%rd42, %r8;
	not.b64 	%rd43, %rd42;
	add.s64 	%rd9, %rd43, %rd41;
	cvta.to.global.u64 	%rd10, %rd36;
	cvta.to.global.u64 	%rd11, %rd32;
	cvta.to.global.u64 	%rd12, %rd33;
	mov.u64 	%rd75, 0;

$L__BB25_2:
	setp.ne.s64 	%p3, %rd3, 0;
	@%p3 bra 	$L__BB25_4;

	// begin inline asm
	trap;
	// end inline asm

$L__BB25_4:
	setp.ne.s32 	%p4, %r4, %r5;
	barrier.sync 	0;
	@%p4 bra 	$L__BB25_7;

	// begin inline asm
	atom.add.release.gpu.u32 %r33,[%rd4],%r6;
	// end inline asm

$L__BB25_6:
	// begin inline asm
	ld.acquire.gpu.u32 %r35,[%rd4];
	// end inline asm
	xor.b32  	%r36, %r35, %r33;
	setp.gt.s32 	%p5, %r36, -1;
	@%p5 bra 	$L__BB25_6;

$L__BB25_7:
	barrier.sync 	0;
	// begin inline asm
	mov.u64 %rd46, %globaltimer;
	// end inline asm
	add.s64 	%rd47, %rd46, 1000000;
	st.global.u64 	[%rd1], %rd47;
	@%p3 bra 	$L__BB25_9;

	// begin inline asm
	trap;
	// end inline asm

$L__BB25_9:
	barrier.sync 	0;
	@%p4 bra 	$L__BB25_12;

	// begin inline asm
	atom.add.release.gpu.u32 %r37,[%rd4],%r6;
	// end inline asm

$L__BB25_11:
	// begin inline asm
	ld.acquire.gpu.u32 %r39,[%rd4];
	// end inline asm
	xor.b32  	%r40, %r39, %r37;
	setp.gt.s32 	%p8, %r40, -1;
	@%p8 bra 	$L__BB25_11;

$L__BB25_12:
	barrier.sync 	0;
	bar.sync 	0;
	ld.global.u64 	%rd14, [%rd1];

$L__BB25_13:
	// begin inline asm
	mov.u64 %rd50, %globaltimer;
	// end inline asm
	setp.lt.s64 	%p9, %rd50, %rd14;
	@%p9 bra 	$L__BB25_13;

	setp.ge.u64 	%p10, %rd2, %rd34;
	@%p10 bra 	$L__BB25_17;

	mov.u64 	%rd76, %rd2;

$L__BB25_16:
	shl.b64 	%rd51, %rd76, 3;
	add.s64 	%rd52, %rd11, %rd51;
	ld.global.f64 	%fd1, [%rd52];
	add.s64 	%rd53, %rd12, %rd51;
	st.global.f64 	[%rd53], %fd1;
	add.s64 	%rd76, %rd76, %rd5;
	setp.lt.u64 	%p11, %rd76, %rd34;
	@%p11 bra 	$L__BB25_16;

$L__BB25_17:
	// begin inline asm
	mov.u64 %rd54, %globaltimer;
	// end inline asm
	sub.s64 	%rd55, %rd54, %rd14;
	st.shared.u64 	[%r7], %rd55;
	@%p3 bra 	$L__BB25_19;

	// begin inline asm
	trap;
	// end inline asm

$L__BB25_19:
	barrier.sync 	0;
	@%p4 bra 	$L__BB25_22;

	// begin inline asm
	atom.add.release.gpu.u32 %r41,[%rd4],%r6;
	// end inline asm

$L__BB25_21:
	// begin inline asm
	ld.acquire.gpu.u32 %r43,[%rd4];
	// end inline asm
	xor.b32  	%r44, %r43, %r41;
	setp.gt.s32 	%p14, %r44, -1;
	@%p14 bra 	$L__BB25_21;

$L__BB25_22:
	setp.ne.s32 	%p15, %r3, 0;
	barrier.sync 	0;
	@%p15 bra 	$L__BB25_32;

	setp.lt.u32 	%p16, %r1, 2;
	ld.shared.u64 	%rd83, [_ZZ23device_copy_kernel_syncPdS_mmPlS0_E6clocks];
	@%p16 bra 	$L__BB25_31;

	setp.lt.u64 	%p17, %rd8, 3;
	mov.u64 	%rd81, 1;
	@%p17 bra 	$L__BB25_27;

	mov.u64 	%rd79, %rd9;

$L__BB25_26:
	cvt.u32.u64 	%r45, %rd81;
	shl.b32 	%r46, %r45, 3;
	add.s32 	%r48, %r30, %r46;
	ld.shared.u64 	%rd61, [%r48];
	max.s64 	%rd62, %rd83, %rd61;
	ld.shared.u64 	%rd63, [%r48+8];
	max.s64 	%rd64, %rd62, %rd63;
	ld.shared.u64 	%rd65, [%r48+16];
	max.s64 	%rd66, %rd64, %rd65;
	ld.shared.u64 	%rd67, [%r48+24];
	max.s64 	%rd83, %rd66, %rd67;
	add.s64 	%rd81, %rd81, 4;
	add.s64 	%rd79, %rd79, -4;
	setp.ne.s64 	%p18, %rd79, 0;
	@%p18 bra 	$L__BB25_26;

$L__BB25_27:
	setp.eq.s32 	%p19, %r8, 0;
	@%p19 bra 	$L__BB25_31;

	setp.eq.s32 	%p20, %r8, 1;
	cvt.u32.u64 	%r49, %rd81;
	shl.b32 	%r50, %r49, 3;
	add.s32 	%r12, %r30, %r50;
	ld.shared.u64 	%rd68, [%r12];
	max.s64 	%rd83, %rd83, %rd68;
	@%p20 bra 	$L__BB25_31;

	setp.eq.s32 	%p21, %r8, 2;
	ld.shared.u64 	%rd69, [%r12+8];
	max.s64 	%rd83, %rd83, %rd69;
	@%p21 bra 	$L__BB25_31;

	ld.shared.u64 	%rd70, [%r12+16];
	max.s64 	%rd83, %rd83, %rd70;

$L__BB25_31:
	mul.lo.s64 	%rd71, %rd75, %rd6;
	add.s64 	%rd72, %rd71, %rd7;
	shl.b64 	%rd73, %rd72, 3;
	add.s64 	%rd74, %rd10, %rd73;
	st.global.u64 	[%rd74], %rd83;

$L__BB25_32:
	add.s64 	%rd75, %rd75, 1;
	setp.lt.u64 	%p22, %rd75, %rd35;
	@%p22 bra 	$L__BB25_2;

$L__BB25_33:
	ret;

}
	// .globl	_Z24device_read_kernel_blockPmmmPl
.visible .entry _Z24device_read_kernel_blockPmmmPl(
	.param .u64 _Z24device_read_kernel_blockPmmmPl_param_0,
	.param .u64 _Z24device_read_kernel_blockPmmmPl_param_1,
	.param .u64 _Z24device_read_kernel_blockPmmmPl_param_2,
	.param .u64 _Z24device_read_kernel_blockPmmmPl_param_3
)
{
	.local .align 8 .b8 	__local_depot26[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<19>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<137>;
	// demoted variable
	.shared .align 8 .b8 _ZZ24device_read_kernel_blockPmmmPlE6clocks[8200];

	mov.u64 	%SPL, __local_depot26;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd43, [_Z24device_read_kernel_blockPmmmPl_param_0];
	ld.param.u64 	%rd44, [_Z24device_read_kernel_blockPmmmPl_param_1];
	ld.param.u64 	%rd45, [_Z24device_read_kernel_blockPmmmPl_param_2];
	ld.param.u64 	%rd46, [_Z24device_read_kernel_blockPmmmPl_param_3];
	cvta.to.global.u64 	%rd1, %rd43;
	mov.u32 	%r5, %ctaid.x;
	setp.eq.s32 	%p1, %r5, 0;
	@%p1 bra 	$L__BB26_2;

	mov.u64 	%rd47, $str$3;
	cvta.global.u64 	%rd48, %rd47;
	mov.u64 	%rd49, $str$4;
	cvta.global.u64 	%rd50, %rd49;
	mov.u64 	%rd51, __unnamed_1;
	cvta.global.u64 	%rd52, %rd51;
	mov.u32 	%r6, 295;
	mov.u64 	%rd53, 1;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd48;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd50;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r6;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd52;
	.param .b64 param4;
	st.param.b64 	[param4+0], %rd53;
	call.uni 
	__assertfail, 
	(
	param0, 
	param1, 
	param2, 
	param3, 
	param4
	);
	} // callseq 2

$L__BB26_2:
	mov.u32 	%r7, %tid.x;
	cvt.u64.u32 	%rd2, %r7;
	setp.ge.u64 	%p2, %rd2, %rd44;
	@%p2 bra 	$L__BB26_5;

	mov.u64 	%rd119, %rd2;

$L__BB26_4:
	shl.b64 	%rd56, %rd119, 3;
	add.s64 	%rd57, %rd1, %rd56;
	ld.global.u64 	%rd58, [%rd57];
	xor.b64  	%rd128, %rd58, %rd128;
	add.s64 	%rd119, %rd119, 1024;
	setp.lt.u64 	%p3, %rd119, %rd44;
	@%p3 bra 	$L__BB26_4;

$L__BB26_5:
	cvt.u32.u64 	%r8, %rd2;
	shl.b32 	%r9, %r8, 3;
	mov.u32 	%r10, _ZZ24device_read_kernel_blockPmmmPlE6clocks;
	add.s32 	%r1, %r10, %r9;
	setp.eq.s64 	%p4, %rd45, 0;
	@%p4 bra 	$L__BB26_27;

	shl.b64 	%rd60, %rd2, 3;
	add.s64 	%rd8, %rd1, %rd60;
	add.s64 	%rd9, %rd43, %rd60;
	shl.b64 	%rd61, %rd44, 3;
	add.s64 	%rd10, %rd1, %rd61;
	mov.u32 	%r2, %ntid.x;
	cvt.u64.u32 	%rd62, %r2;
	add.s64 	%rd11, %rd62, -2;
	add.s32 	%r11, %r2, -1;
	and.b32  	%r3, %r11, 3;
	cvt.u64.u32 	%rd63, %r3;
	not.b64 	%rd64, %rd63;
	add.s64 	%rd12, %rd64, %rd62;
	cvta.to.global.u64 	%rd13, %rd46;
	mov.u64 	%rd122, 0;

$L__BB26_7:
	setp.ne.s32 	%p5, %r8, 0;
	bar.sync 	0;
	@%p5 bra 	$L__BB26_9;

	// begin inline asm
	mov.u64 	%rd65, %clock64;
	// end inline asm
	add.s64 	%rd66, %rd65, 19799997;
	st.shared.u64 	[_ZZ24device_read_kernel_blockPmmmPlE6clocks+8192], %rd66;

$L__BB26_9:
	bar.sync 	0;
	ld.shared.u64 	%rd16, [_ZZ24device_read_kernel_blockPmmmPlE6clocks+8192];

$L__BB26_10:
	// begin inline asm
	mov.u64 	%rd67, %clock64;
	// end inline asm
	setp.lt.s64 	%p6, %rd67, %rd16;
	@%p6 bra 	$L__BB26_10;

	mov.u64 	%rd124, 0;

$L__BB26_12:
	setp.ge.s64 	%p7, %rd2, %rd44;
	@%p7 bra 	$L__BB26_15;

	mov.u64 	%rd125, %rd9;
	mov.u64 	%rd126, %rd8;

$L__BB26_14:
	// begin inline asm
	ld.ca.u64 %rd69, [%rd125];
	// end inline asm
	xor.b64  	%rd85, %rd69, %rd128;
	add.s64 	%rd72, %rd125, 8192;
	// begin inline asm
	ld.ca.u64 %rd71, [%rd72];
	// end inline asm
	xor.b64  	%rd86, %rd71, %rd85;
	add.s64 	%rd74, %rd125, 16384;
	// begin inline asm
	ld.ca.u64 %rd73, [%rd74];
	// end inline asm
	xor.b64  	%rd87, %rd73, %rd86;
	add.s64 	%rd76, %rd125, 24576;
	// begin inline asm
	ld.ca.u64 %rd75, [%rd76];
	// end inline asm
	xor.b64  	%rd88, %rd75, %rd87;
	add.s64 	%rd78, %rd125, 32768;
	// begin inline asm
	ld.ca.u64 %rd77, [%rd78];
	// end inline asm
	xor.b64  	%rd89, %rd77, %rd88;
	add.s64 	%rd80, %rd125, 40960;
	// begin inline asm
	ld.ca.u64 %rd79, [%rd80];
	// end inline asm
	xor.b64  	%rd90, %rd79, %rd89;
	add.s64 	%rd82, %rd125, 49152;
	// begin inline asm
	ld.ca.u64 %rd81, [%rd82];
	// end inline asm
	xor.b64  	%rd91, %rd81, %rd90;
	add.s64 	%rd84, %rd125, 57344;
	// begin inline asm
	ld.ca.u64 %rd83, [%rd84];
	// end inline asm
	xor.b64  	%rd128, %rd83, %rd91;
	add.s64 	%rd126, %rd126, 65536;
	setp.lt.u64 	%p8, %rd126, %rd10;
	add.s64 	%rd125, %rd125, 65536;
	@%p8 bra 	$L__BB26_14;

$L__BB26_15:
	add.s64 	%rd124, %rd124, 1;
	setp.lt.u64 	%p9, %rd124, 8192;
	@%p9 bra 	$L__BB26_12;

	// begin inline asm
	mov.u64 	%rd92, %clock64;
	// end inline asm
	st.shared.u64 	[%r1], %rd92;
	bar.sync 	0;
	@%p5 bra 	$L__BB26_26;

	setp.lt.u32 	%p11, %r2, 2;
	ld.shared.u64 	%rd135, [_ZZ24device_read_kernel_blockPmmmPlE6clocks];
	@%p11 bra 	$L__BB26_25;

	setp.lt.u64 	%p12, %rd11, 3;
	mov.u64 	%rd133, 1;
	@%p12 bra 	$L__BB26_21;

	mov.u64 	%rd131, %rd12;

$L__BB26_20:
	cvt.u32.u64 	%r14, %rd133;
	shl.b32 	%r15, %r14, 3;
	add.s32 	%r17, %r10, %r15;
	ld.shared.u64 	%rd96, [%r17];
	max.s64 	%rd97, %rd135, %rd96;
	ld.shared.u64 	%rd98, [%r17+8];
	max.s64 	%rd99, %rd97, %rd98;
	ld.shared.u64 	%rd100, [%r17+16];
	max.s64 	%rd101, %rd99, %rd100;
	ld.shared.u64 	%rd102, [%r17+24];
	max.s64 	%rd135, %rd101, %rd102;
	add.s64 	%rd133, %rd133, 4;
	add.s64 	%rd131, %rd131, -4;
	setp.ne.s64 	%p13, %rd131, 0;
	@%p13 bra 	$L__BB26_20;

$L__BB26_21:
	setp.eq.s32 	%p14, %r3, 0;
	@%p14 bra 	$L__BB26_25;

	setp.eq.s32 	%p15, %r3, 1;
	cvt.u32.u64 	%r18, %rd133;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r4, %r10, %r19;
	ld.shared.u64 	%rd103, [%r4];
	max.s64 	%rd135, %rd135, %rd103;
	@%p15 bra 	$L__BB26_25;

	setp.eq.s32 	%p16, %r3, 2;
	ld.shared.u64 	%rd104, [%r4+8];
	max.s64 	%rd135, %rd135, %rd104;
	@%p16 bra 	$L__BB26_25;

	ld.shared.u64 	%rd105, [%r4+16];
	max.s64 	%rd135, %rd135, %rd105;

$L__BB26_25:
	sub.s64 	%rd106, %rd135, %rd16;
	shr.s64 	%rd107, %rd106, 63;
	shr.u64 	%rd108, %rd107, 51;
	add.s64 	%rd109, %rd106, %rd108;
	shr.s64 	%rd110, %rd109, 13;
	shl.b64 	%rd111, %rd122, 3;
	add.s64 	%rd112, %rd13, %rd111;
	st.global.u64 	[%rd112], %rd110;

$L__BB26_26:
	add.s64 	%rd122, %rd122, 1;
	setp.lt.u64 	%p17, %rd122, %rd45;
	@%p17 bra 	$L__BB26_7;

$L__BB26_27:
	ld.shared.u64 	%rd113, [%r1];
	setp.ge.u64 	%p18, %rd113, %rd2;
	@%p18 bra 	$L__BB26_29;

	add.u64 	%rd114, %SP, 0;
	add.u64 	%rd115, %SPL, 0;
	st.local.u64 	[%rd115], %rd128;
	mov.u64 	%rd116, $str$2;
	cvta.global.u64 	%rd117, %rd116;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd117;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd114;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r21, [retval0+0];
	} // callseq 3

$L__BB26_29:
	ret;

}
	// .globl	_Z25device_write_kernel_blockPdmmPl
.visible .entry _Z25device_write_kernel_blockPdmmPl(
	.param .u64 _Z25device_write_kernel_blockPdmmPl_param_0,
	.param .u64 _Z25device_write_kernel_blockPdmmPl_param_1,
	.param .u64 _Z25device_write_kernel_blockPdmmPl_param_2,
	.param .u64 _Z25device_write_kernel_blockPdmmPl_param_3
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<66>;
	// demoted variable
	.shared .align 8 .b8 _ZZ25device_write_kernel_blockPdmmPlE6clocks[8200];

	ld.param.u64 	%rd26, [_Z25device_write_kernel_blockPdmmPl_param_0];
	ld.param.u64 	%rd27, [_Z25device_write_kernel_blockPdmmPl_param_1];
	ld.param.u64 	%rd28, [_Z25device_write_kernel_blockPdmmPl_param_2];
	ld.param.u64 	%rd29, [_Z25device_write_kernel_blockPdmmPl_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r8, %r7, %r1, %r2;
	cvt.u64.u32 	%rd1, %r8;
	setp.eq.s64 	%p1, %rd28, 0;
	@%p1 bra 	$L__BB27_20;

	cvt.u32.u64 	%r3, %rd1;
	shl.b32 	%r9, %r2, 3;
	mov.u32 	%r10, _ZZ25device_write_kernel_blockPdmmPlE6clocks;
	add.s32 	%r4, %r10, %r9;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r12, %r1, %r11;
	cvt.u64.u32 	%rd2, %r12;
	cvt.u64.u32 	%rd31, %r1;
	add.s64 	%rd3, %rd31, -2;
	add.s32 	%r13, %r1, -1;
	and.b32  	%r5, %r13, 3;
	cvt.u64.u32 	%rd32, %r5;
	not.b64 	%rd33, %rd32;
	add.s64 	%rd4, %rd33, %rd31;
	cvta.to.global.u64 	%rd5, %rd29;
	cvta.to.global.u64 	%rd6, %rd26;
	mov.u64 	%rd57, 0;

$L__BB27_2:
	bar.sync 	0;
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB27_4;

	// begin inline asm
	mov.u64 	%rd34, %clock64;
	// end inline asm
	add.s64 	%rd35, %rd34, 19799997;
	st.shared.u64 	[_ZZ25device_write_kernel_blockPdmmPlE6clocks+8192], %rd35;

$L__BB27_4:
	bar.sync 	0;
	ld.shared.u64 	%rd8, [_ZZ25device_write_kernel_blockPdmmPlE6clocks+8192];

$L__BB27_5:
	// begin inline asm
	mov.u64 	%rd36, %clock64;
	// end inline asm
	setp.lt.s64 	%p3, %rd36, %rd8;
	@%p3 bra 	$L__BB27_5;

	setp.ge.u64 	%p4, %rd1, %rd27;
	@%p4 bra 	$L__BB27_9;

	mov.u64 	%rd58, %rd1;

$L__BB27_8:
	shl.b64 	%rd37, %rd58, 3;
	add.s64 	%rd38, %rd6, %rd37;
	mov.u64 	%rd39, 0;
	st.global.u64 	[%rd38], %rd39;
	add.s64 	%rd58, %rd58, %rd2;
	setp.lt.u64 	%p5, %rd58, %rd27;
	@%p5 bra 	$L__BB27_8;

$L__BB27_9:
	// begin inline asm
	mov.u64 	%rd40, %clock64;
	// end inline asm
	sub.s64 	%rd41, %rd40, %rd8;
	st.shared.u64 	[%r4], %rd41;
	bar.sync 	0;
	@%p2 bra 	$L__BB27_19;

	setp.lt.u32 	%p7, %r1, 2;
	ld.shared.u64 	%rd65, [_ZZ25device_write_kernel_blockPdmmPlE6clocks];
	@%p7 bra 	$L__BB27_18;

	setp.lt.u64 	%p8, %rd3, 3;
	mov.u64 	%rd63, 1;
	@%p8 bra 	$L__BB27_14;

	mov.u64 	%rd61, %rd4;

$L__BB27_13:
	cvt.u32.u64 	%r14, %rd63;
	shl.b32 	%r15, %r14, 3;
	add.s32 	%r17, %r10, %r15;
	ld.shared.u64 	%rd45, [%r17];
	max.s64 	%rd46, %rd65, %rd45;
	ld.shared.u64 	%rd47, [%r17+8];
	max.s64 	%rd48, %rd46, %rd47;
	ld.shared.u64 	%rd49, [%r17+16];
	max.s64 	%rd50, %rd48, %rd49;
	ld.shared.u64 	%rd51, [%r17+24];
	max.s64 	%rd65, %rd50, %rd51;
	add.s64 	%rd63, %rd63, 4;
	add.s64 	%rd61, %rd61, -4;
	setp.ne.s64 	%p9, %rd61, 0;
	@%p9 bra 	$L__BB27_13;

$L__BB27_14:
	setp.eq.s32 	%p10, %r5, 0;
	@%p10 bra 	$L__BB27_18;

	setp.eq.s32 	%p11, %r5, 1;
	cvt.u32.u64 	%r18, %rd63;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r6, %r10, %r19;
	ld.shared.u64 	%rd52, [%r6];
	max.s64 	%rd65, %rd65, %rd52;
	@%p11 bra 	$L__BB27_18;

	setp.eq.s32 	%p12, %r5, 2;
	ld.shared.u64 	%rd53, [%r6+8];
	max.s64 	%rd65, %rd65, %rd53;
	@%p12 bra 	$L__BB27_18;

	ld.shared.u64 	%rd54, [%r6+16];
	max.s64 	%rd65, %rd65, %rd54;

$L__BB27_18:
	shl.b64 	%rd55, %rd57, 3;
	add.s64 	%rd56, %rd5, %rd55;
	st.global.u64 	[%rd56], %rd65;

$L__BB27_19:
	add.s64 	%rd57, %rd57, 1;
	setp.lt.u64 	%p13, %rd57, %rd28;
	@%p13 bra 	$L__BB27_2;

$L__BB27_20:
	ret;

}
	// .globl	_Z24device_copy_kernel_blockPdS_mmPl
.visible .entry _Z24device_copy_kernel_blockPdS_mmPl(
	.param .u64 _Z24device_copy_kernel_blockPdS_mmPl_param_0,
	.param .u64 _Z24device_copy_kernel_blockPdS_mmPl_param_1,
	.param .u64 _Z24device_copy_kernel_blockPdS_mmPl_param_2,
	.param .u64 _Z24device_copy_kernel_blockPdS_mmPl_param_3,
	.param .u64 _Z24device_copy_kernel_blockPdS_mmPl_param_4
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<21>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<68>;
	// demoted variable
	.shared .align 8 .b8 _ZZ24device_copy_kernel_blockPdS_mmPlE6clocks[8200];

	ld.param.u64 	%rd27, [_Z24device_copy_kernel_blockPdS_mmPl_param_0];
	ld.param.u64 	%rd28, [_Z24device_copy_kernel_blockPdS_mmPl_param_1];
	ld.param.u64 	%rd29, [_Z24device_copy_kernel_blockPdS_mmPl_param_2];
	ld.param.u64 	%rd30, [_Z24device_copy_kernel_blockPdS_mmPl_param_3];
	ld.param.u64 	%rd31, [_Z24device_copy_kernel_blockPdS_mmPl_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r8, %r7, %r1, %r2;
	cvt.u64.u32 	%rd1, %r8;
	setp.eq.s64 	%p1, %rd30, 0;
	@%p1 bra 	$L__BB28_20;

	cvt.u32.u64 	%r3, %rd1;
	shl.b32 	%r9, %r2, 3;
	mov.u32 	%r10, _ZZ24device_copy_kernel_blockPdS_mmPlE6clocks;
	add.s32 	%r4, %r10, %r9;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r12, %r1, %r11;
	cvt.u64.u32 	%rd2, %r12;
	cvt.u64.u32 	%rd33, %r1;
	add.s64 	%rd3, %rd33, -2;
	add.s32 	%r13, %r1, -1;
	and.b32  	%r5, %r13, 3;
	cvt.u64.u32 	%rd34, %r5;
	not.b64 	%rd35, %rd34;
	add.s64 	%rd4, %rd35, %rd33;
	cvta.to.global.u64 	%rd5, %rd31;
	cvta.to.global.u64 	%rd6, %rd27;
	cvta.to.global.u64 	%rd7, %rd28;
	mov.u64 	%rd59, 0;

$L__BB28_2:
	bar.sync 	0;
	setp.ne.s32 	%p2, %r3, 0;
	@%p2 bra 	$L__BB28_4;

	// begin inline asm
	mov.u64 	%rd36, %clock64;
	// end inline asm
	add.s64 	%rd37, %rd36, 19799997;
	st.shared.u64 	[_ZZ24device_copy_kernel_blockPdS_mmPlE6clocks+8192], %rd37;

$L__BB28_4:
	bar.sync 	0;
	ld.shared.u64 	%rd9, [_ZZ24device_copy_kernel_blockPdS_mmPlE6clocks+8192];

$L__BB28_5:
	// begin inline asm
	mov.u64 	%rd38, %clock64;
	// end inline asm
	setp.lt.s64 	%p3, %rd38, %rd9;
	@%p3 bra 	$L__BB28_5;

	setp.ge.u64 	%p4, %rd1, %rd29;
	@%p4 bra 	$L__BB28_9;

	mov.u64 	%rd60, %rd1;

$L__BB28_8:
	shl.b64 	%rd39, %rd60, 3;
	add.s64 	%rd40, %rd6, %rd39;
	ld.global.f64 	%fd1, [%rd40];
	add.s64 	%rd41, %rd7, %rd39;
	st.global.f64 	[%rd41], %fd1;
	add.s64 	%rd60, %rd60, %rd2;
	setp.lt.u64 	%p5, %rd60, %rd29;
	@%p5 bra 	$L__BB28_8;

$L__BB28_9:
	// begin inline asm
	mov.u64 	%rd42, %clock64;
	// end inline asm
	sub.s64 	%rd43, %rd42, %rd9;
	st.shared.u64 	[%r4], %rd43;
	bar.sync 	0;
	@%p2 bra 	$L__BB28_19;

	setp.lt.u32 	%p7, %r1, 2;
	ld.shared.u64 	%rd67, [_ZZ24device_copy_kernel_blockPdS_mmPlE6clocks];
	@%p7 bra 	$L__BB28_18;

	setp.lt.u64 	%p8, %rd3, 3;
	mov.u64 	%rd65, 1;
	@%p8 bra 	$L__BB28_14;

	mov.u64 	%rd63, %rd4;

$L__BB28_13:
	cvt.u32.u64 	%r14, %rd65;
	shl.b32 	%r15, %r14, 3;
	add.s32 	%r17, %r10, %r15;
	ld.shared.u64 	%rd47, [%r17];
	max.s64 	%rd48, %rd67, %rd47;
	ld.shared.u64 	%rd49, [%r17+8];
	max.s64 	%rd50, %rd48, %rd49;
	ld.shared.u64 	%rd51, [%r17+16];
	max.s64 	%rd52, %rd50, %rd51;
	ld.shared.u64 	%rd53, [%r17+24];
	max.s64 	%rd67, %rd52, %rd53;
	add.s64 	%rd65, %rd65, 4;
	add.s64 	%rd63, %rd63, -4;
	setp.ne.s64 	%p9, %rd63, 0;
	@%p9 bra 	$L__BB28_13;

$L__BB28_14:
	setp.eq.s32 	%p10, %r5, 0;
	@%p10 bra 	$L__BB28_18;

	setp.eq.s32 	%p11, %r5, 1;
	cvt.u32.u64 	%r18, %rd65;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r6, %r10, %r19;
	ld.shared.u64 	%rd54, [%r6];
	max.s64 	%rd67, %rd67, %rd54;
	@%p11 bra 	$L__BB28_18;

	setp.eq.s32 	%p12, %r5, 2;
	ld.shared.u64 	%rd55, [%r6+8];
	max.s64 	%rd67, %rd67, %rd55;
	@%p12 bra 	$L__BB28_18;

	ld.shared.u64 	%rd56, [%r6+16];
	max.s64 	%rd67, %rd67, %rd56;

$L__BB28_18:
	shl.b64 	%rd57, %rd59, 3;
	add.s64 	%rd58, %rd5, %rd57;
	st.global.u64 	[%rd58], %rd67;

$L__BB28_19:
	add.s64 	%rd59, %rd59, 1;
	setp.lt.u64 	%p13, %rd59, %rd30;
	@%p13 bra 	$L__BB28_2;

$L__BB28_20:
	ret;

}
	// .globl	_Z25device_read_kernel_singlePdmmPVl
.visible .entry _Z25device_read_kernel_singlePdmmPVl(
	.param .u64 _Z25device_read_kernel_singlePdmmPVl_param_0,
	.param .u64 _Z25device_read_kernel_singlePdmmPVl_param_1,
	.param .u64 _Z25device_read_kernel_singlePdmmPVl_param_2,
	.param .u64 _Z25device_read_kernel_singlePdmmPVl_param_3
)
{
	.local .align 16 .b8 	__local_depot29[80];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<6>;
	.reg .b32 	%r<2>;
	.reg .f64 	%fd<67>;
	.reg .b64 	%rd<35>;


	mov.u64 	%SPL, __local_depot29;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd12, [_Z25device_read_kernel_singlePdmmPVl_param_0];
	ld.param.u64 	%rd13, [_Z25device_read_kernel_singlePdmmPVl_param_1];
	ld.param.u64 	%rd14, [_Z25device_read_kernel_singlePdmmPVl_param_2];
	ld.param.u64 	%rd15, [_Z25device_read_kernel_singlePdmmPVl_param_3];
	cvta.to.global.u64 	%rd1, %rd15;
	add.u64 	%rd2, %SPL, 0;
	setp.eq.s64 	%p1, %rd14, 0;
	@%p1 bra 	$L__BB29_9;

	add.u64 	%rd18, %SP, 64;
	add.u64 	%rd3, %SPL, 64;
	cvta.to.global.u64 	%rd4, %rd12;
	mov.u64 	%rd32, 0;

$L__BB29_2:
	// begin inline asm
	mov.u64 	%rd19, %clock64;
	// end inline asm
	setp.eq.s64 	%p2, %rd13, 0;
	@%p2 bra 	$L__BB29_6;

	mov.u64 	%rd34, 0;
	mov.u64 	%rd33, %rd4;

$L__BB29_4:
	ld.global.f64 	%fd34, [%rd33];
	add.f64 	%fd66, %fd34, %fd66;
	ld.global.f64 	%fd35, [%rd33+8];
	add.f64 	%fd65, %fd35, %fd65;
	ld.global.f64 	%fd36, [%rd33+16];
	add.f64 	%fd64, %fd36, %fd64;
	ld.global.f64 	%fd37, [%rd33+24];
	add.f64 	%fd63, %fd37, %fd63;
	ld.global.f64 	%fd38, [%rd33+32];
	add.f64 	%fd62, %fd38, %fd62;
	ld.global.f64 	%fd39, [%rd33+40];
	add.f64 	%fd61, %fd39, %fd61;
	ld.global.f64 	%fd40, [%rd33+48];
	add.f64 	%fd60, %fd40, %fd60;
	ld.global.f64 	%fd41, [%rd33+56];
	add.f64 	%fd59, %fd41, %fd59;
	add.s64 	%rd33, %rd33, 64;
	add.s64 	%rd34, %rd34, 8;
	setp.lt.u64 	%p3, %rd34, %rd13;
	@%p3 bra 	$L__BB29_4;

	st.local.v2.f64 	[%rd2], {%fd66, %fd65};
	st.local.v2.f64 	[%rd2+16], {%fd64, %fd63};
	st.local.v2.f64 	[%rd2+32], {%fd62, %fd61};
	st.local.v2.f64 	[%rd2+48], {%fd60, %fd59};

$L__BB29_6:
	// begin inline asm
	mov.u64 	%rd21, %clock64;
	// end inline asm
	sub.s64 	%rd22, %rd21, %rd19;
	shl.b64 	%rd23, %rd32, 3;
	add.s64 	%rd24, %rd1, %rd23;
	st.volatile.global.u64 	[%rd24], %rd22;
	ld.volatile.global.u64 	%rd25, [%rd1];
	setp.gt.s64 	%p4, %rd25, 7;
	@%p4 bra 	$L__BB29_8;

	ld.volatile.global.u64 	%rd26, [%rd1];
	shl.b64 	%rd27, %rd26, 3;
	add.s64 	%rd28, %rd2, %rd27;
	ld.local.f64 	%fd42, [%rd28];
	st.local.f64 	[%rd3], %fd42;
	mov.u64 	%rd29, $str$1;
	cvta.global.u64 	%rd30, %rd29;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd30;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd18;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r1, [retval0+0];
	} // callseq 4

$L__BB29_8:
	add.s64 	%rd32, %rd32, 1;
	setp.lt.u64 	%p5, %rd32, %rd14;
	@%p5 bra 	$L__BB29_2;

$L__BB29_9:
	ret;

}
	// .globl	_Z26device_write_kernel_singlePdmmPl
.visible .entry _Z26device_write_kernel_singlePdmmPl(
	.param .u64 _Z26device_write_kernel_singlePdmmPl_param_0,
	.param .u64 _Z26device_write_kernel_singlePdmmPl_param_1,
	.param .u64 _Z26device_write_kernel_singlePdmmPl_param_2,
	.param .u64 _Z26device_write_kernel_singlePdmmPl_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd10, [_Z26device_write_kernel_singlePdmmPl_param_0];
	ld.param.u64 	%rd11, [_Z26device_write_kernel_singlePdmmPl_param_1];
	ld.param.u64 	%rd12, [_Z26device_write_kernel_singlePdmmPl_param_2];
	ld.param.u64 	%rd13, [_Z26device_write_kernel_singlePdmmPl_param_3];
	setp.eq.s64 	%p1, %rd12, 0;
	@%p1 bra 	$L__BB30_6;

	cvta.to.global.u64 	%rd1, %rd13;
	cvta.to.global.u64 	%rd2, %rd10;
	mov.u64 	%rd21, 0;

$L__BB30_2:
	// begin inline asm
	mov.u64 	%rd15, %clock64;
	// end inline asm
	setp.eq.s64 	%p2, %rd11, 0;
	@%p2 bra 	$L__BB30_5;

	mov.u64 	%rd23, 0;
	mov.u64 	%rd22, %rd2;

$L__BB30_4:
	cvt.rn.f64.u64 	%fd1, %rd23;
	st.global.f64 	[%rd22], %fd1;
	st.global.f64 	[%rd22+8], %fd1;
	st.global.f64 	[%rd22+16], %fd1;
	st.global.f64 	[%rd22+24], %fd1;
	st.global.f64 	[%rd22+32], %fd1;
	st.global.f64 	[%rd22+40], %fd1;
	st.global.f64 	[%rd22+48], %fd1;
	st.global.f64 	[%rd22+56], %fd1;
	add.s64 	%rd22, %rd22, 64;
	add.s64 	%rd23, %rd23, 8;
	setp.lt.u64 	%p3, %rd23, %rd11;
	@%p3 bra 	$L__BB30_4;

$L__BB30_5:
	// begin inline asm
	mov.u64 	%rd17, %clock64;
	// end inline asm
	sub.s64 	%rd18, %rd17, %rd15;
	shl.b64 	%rd19, %rd21, 3;
	add.s64 	%rd20, %rd1, %rd19;
	st.global.u64 	[%rd20], %rd18;
	add.s64 	%rd21, %rd21, 1;
	setp.lt.u64 	%p4, %rd21, %rd12;
	@%p4 bra 	$L__BB30_2;

$L__BB30_6:
	ret;

}
	// .globl	_Z25device_copy_kernel_singlePdS_mmPl
.visible .entry _Z25device_copy_kernel_singlePdS_mmPl(
	.param .u64 _Z25device_copy_kernel_singlePdS_mmPl_param_0,
	.param .u64 _Z25device_copy_kernel_singlePdS_mmPl_param_1,
	.param .u64 _Z25device_copy_kernel_singlePdS_mmPl_param_2,
	.param .u64 _Z25device_copy_kernel_singlePdS_mmPl_param_3,
	.param .u64 _Z25device_copy_kernel_singlePdS_mmPl_param_4
)
{
	.reg .pred 	%p<5>;
	.reg .f64 	%fd<9>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd13, [_Z25device_copy_kernel_singlePdS_mmPl_param_0];
	ld.param.u64 	%rd14, [_Z25device_copy_kernel_singlePdS_mmPl_param_1];
	ld.param.u64 	%rd15, [_Z25device_copy_kernel_singlePdS_mmPl_param_2];
	ld.param.u64 	%rd16, [_Z25device_copy_kernel_singlePdS_mmPl_param_3];
	ld.param.u64 	%rd17, [_Z25device_copy_kernel_singlePdS_mmPl_param_4];
	setp.eq.s64 	%p1, %rd16, 0;
	@%p1 bra 	$L__BB31_6;

	cvta.to.global.u64 	%rd1, %rd17;
	cvta.to.global.u64 	%rd2, %rd13;
	cvta.to.global.u64 	%rd3, %rd14;
	mov.u64 	%rd25, 0;

$L__BB31_2:
	// begin inline asm
	mov.u64 	%rd19, %clock64;
	// end inline asm
	setp.eq.s64 	%p2, %rd15, 0;
	@%p2 bra 	$L__BB31_5;

	mov.u64 	%rd28, 0;
	mov.u64 	%rd26, %rd2;
	mov.u64 	%rd27, %rd3;

$L__BB31_4:
	ld.global.f64 	%fd1, [%rd26];
	st.global.f64 	[%rd27], %fd1;
	ld.global.f64 	%fd2, [%rd26+8];
	st.global.f64 	[%rd27+8], %fd2;
	ld.global.f64 	%fd3, [%rd26+16];
	st.global.f64 	[%rd27+16], %fd3;
	ld.global.f64 	%fd4, [%rd26+24];
	st.global.f64 	[%rd27+24], %fd4;
	ld.global.f64 	%fd5, [%rd26+32];
	st.global.f64 	[%rd27+32], %fd5;
	ld.global.f64 	%fd6, [%rd26+40];
	st.global.f64 	[%rd27+40], %fd6;
	ld.global.f64 	%fd7, [%rd26+48];
	st.global.f64 	[%rd27+48], %fd7;
	ld.global.f64 	%fd8, [%rd26+56];
	st.global.f64 	[%rd27+56], %fd8;
	add.s64 	%rd27, %rd27, 64;
	add.s64 	%rd26, %rd26, 64;
	add.s64 	%rd28, %rd28, 8;
	setp.lt.u64 	%p3, %rd28, %rd15;
	@%p3 bra 	$L__BB31_4;

$L__BB31_5:
	// begin inline asm
	mov.u64 	%rd21, %clock64;
	// end inline asm
	sub.s64 	%rd22, %rd21, %rd19;
	shl.b64 	%rd23, %rd25, 3;
	add.s64 	%rd24, %rd1, %rd23;
	st.global.u64 	[%rd24], %rd22;
	add.s64 	%rd25, %rd25, 1;
	setp.lt.u64 	%p4, %rd25, %rd16;
	@%p4 bra 	$L__BB31_2;

$L__BB31_6:
	ret;

}
	// .globl	_Z14latency_kernelPhmmPd
.visible .entry _Z14latency_kernelPhmmPd(
	.param .u64 _Z14latency_kernelPhmmPd_param_0,
	.param .u64 _Z14latency_kernelPhmmPd_param_1,
	.param .u64 _Z14latency_kernelPhmmPd_param_2,
	.param .u64 _Z14latency_kernelPhmmPd_param_3
)
{
	.local .align 8 .b8 	__local_depot32[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<93>;


	mov.u64 	%SPL, __local_depot32;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd20, [_Z14latency_kernelPhmmPd_param_0];
	ld.param.u64 	%rd21, [_Z14latency_kernelPhmmPd_param_1];
	ld.param.u64 	%rd22, [_Z14latency_kernelPhmmPd_param_2];
	ld.param.u64 	%rd23, [_Z14latency_kernelPhmmPd_param_3];
	setp.eq.s64 	%p1, %rd22, 0;
	@%p1 bra 	$L__BB32_13;

	shr.u64 	%rd1, %rd21, 3;
	add.s64 	%rd2, %rd1, -1;
	and.b64  	%rd3, %rd1, 3;
	sub.s64 	%rd4, %rd3, %rd1;
	mov.u32 	%r2, %tid.x;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r2;
	add.u64 	%rd25, %SP, 0;
	add.u64 	%rd5, %SPL, 0;
	cvta.to.global.u64 	%rd6, %rd23;
	mov.u64 	%rd87, 0;

$L__BB32_2:
	// begin inline asm
	mov.u64 	%rd26, %clock64;
	// end inline asm
	setp.eq.s64 	%p2, %rd1, 0;
	mov.u64 	%rd92, %rd20;
	@%p2 bra 	$L__BB32_10;

	setp.lt.u64 	%p3, %rd2, 3;
	mov.u64 	%rd92, %rd20;
	@%p3 bra 	$L__BB32_6;

	mov.u64 	%rd88, %rd4;
	mov.u64 	%rd92, %rd20;

$L__BB32_5:
	ld.u64 	%rd28, [%rd92];
	ld.u64 	%rd29, [%rd28];
	ld.u64 	%rd30, [%rd29];
	ld.u64 	%rd31, [%rd30];
	ld.u64 	%rd32, [%rd31];
	ld.u64 	%rd33, [%rd32];
	ld.u64 	%rd34, [%rd33];
	ld.u64 	%rd35, [%rd34];
	ld.u64 	%rd36, [%rd35];
	ld.u64 	%rd37, [%rd36];
	ld.u64 	%rd38, [%rd37];
	ld.u64 	%rd39, [%rd38];
	ld.u64 	%rd40, [%rd39];
	ld.u64 	%rd41, [%rd40];
	ld.u64 	%rd42, [%rd41];
	ld.u64 	%rd43, [%rd42];
	ld.u64 	%rd44, [%rd43];
	ld.u64 	%rd45, [%rd44];
	ld.u64 	%rd46, [%rd45];
	ld.u64 	%rd47, [%rd46];
	ld.u64 	%rd48, [%rd47];
	ld.u64 	%rd49, [%rd48];
	ld.u64 	%rd50, [%rd49];
	ld.u64 	%rd51, [%rd50];
	ld.u64 	%rd52, [%rd51];
	ld.u64 	%rd53, [%rd52];
	ld.u64 	%rd54, [%rd53];
	ld.u64 	%rd55, [%rd54];
	ld.u64 	%rd56, [%rd55];
	ld.u64 	%rd57, [%rd56];
	ld.u64 	%rd58, [%rd57];
	ld.u64 	%rd92, [%rd58];
	add.s64 	%rd88, %rd88, 4;
	setp.ne.s64 	%p4, %rd88, 0;
	@%p4 bra 	$L__BB32_5;

$L__BB32_6:
	setp.eq.s64 	%p5, %rd3, 0;
	@%p5 bra 	$L__BB32_10;

	setp.eq.s64 	%p6, %rd3, 1;
	ld.u64 	%rd59, [%rd92];
	ld.u64 	%rd60, [%rd59];
	ld.u64 	%rd61, [%rd60];
	ld.u64 	%rd62, [%rd61];
	ld.u64 	%rd63, [%rd62];
	ld.u64 	%rd64, [%rd63];
	ld.u64 	%rd65, [%rd64];
	ld.u64 	%rd92, [%rd65];
	@%p6 bra 	$L__BB32_10;

	setp.eq.s64 	%p7, %rd3, 2;
	ld.u64 	%rd66, [%rd92];
	ld.u64 	%rd67, [%rd66];
	ld.u64 	%rd68, [%rd67];
	ld.u64 	%rd69, [%rd68];
	ld.u64 	%rd70, [%rd69];
	ld.u64 	%rd71, [%rd70];
	ld.u64 	%rd72, [%rd71];
	ld.u64 	%rd92, [%rd72];
	@%p7 bra 	$L__BB32_10;

	ld.u64 	%rd73, [%rd92];
	ld.u64 	%rd74, [%rd73];
	ld.u64 	%rd75, [%rd74];
	ld.u64 	%rd76, [%rd75];
	ld.u64 	%rd77, [%rd76];
	ld.u64 	%rd78, [%rd77];
	ld.u64 	%rd79, [%rd78];
	ld.u64 	%rd92, [%rd79];

$L__BB32_10:
	// begin inline asm
	mov.u64 	%rd80, %clock64;
	// end inline asm
	sub.s64 	%rd81, %rd80, %rd26;
	cvt.rn.f64.s64 	%fd1, %rd81;
	shl.b64 	%rd82, %rd87, 3;
	add.s64 	%rd83, %rd6, %rd82;
	st.global.f64 	[%rd83], %fd1;
	setp.lt.s32 	%p8, %r1, 2;
	@%p8 bra 	$L__BB32_12;

	ld.u8 	%r5, [%rd92];
	st.local.u32 	[%rd5], %r5;
	mov.u64 	%rd84, $str$5;
	cvta.global.u64 	%rd85, %rd84;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd85;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd25;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r6, [retval0+0];
	} // callseq 5

$L__BB32_12:
	add.s64 	%rd87, %rd87, 1;
	setp.lt.u64 	%p9, %rd87, %rd22;
	@%p9 bra 	$L__BB32_2;

$L__BB32_13:
	ret;

}
	// .globl	_Z12cache_kernelPm
.visible .entry _Z12cache_kernelPm(
	.param .u64 _Z12cache_kernelPm_param_0
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<133>;


	ld.param.u64 	%rd1, [_Z12cache_kernelPm_param_0];
	mov.u32 	%r7, %ctaid.x;
	shl.b32 	%r8, %r7, 10;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r16, %r8, %r9;
	mov.u32 	%r15, 0;

$L__BB33_1:
	mul.wide.s32 	%rd130, %r16, 8;
	add.s64 	%rd3, %rd1, %rd130;
	// begin inline asm
	ld.ca.u64 %rd2, [%rd3];
	// end inline asm
	xor.b64  	%rd5, %rd2, %rd3;
	// begin inline asm
	ld.ca.u64 %rd4, [%rd5];
	// end inline asm
	xor.b64  	%rd7, %rd4, %rd5;
	// begin inline asm
	ld.ca.u64 %rd6, [%rd7];
	// end inline asm
	xor.b64  	%rd9, %rd6, %rd7;
	// begin inline asm
	ld.ca.u64 %rd8, [%rd9];
	// end inline asm
	xor.b64  	%rd11, %rd8, %rd9;
	// begin inline asm
	ld.ca.u64 %rd10, [%rd11];
	// end inline asm
	xor.b64  	%rd13, %rd10, %rd11;
	// begin inline asm
	ld.ca.u64 %rd12, [%rd13];
	// end inline asm
	xor.b64  	%rd15, %rd12, %rd13;
	// begin inline asm
	ld.ca.u64 %rd14, [%rd15];
	// end inline asm
	xor.b64  	%rd17, %rd14, %rd15;
	// begin inline asm
	ld.ca.u64 %rd16, [%rd17];
	// end inline asm
	xor.b64  	%rd19, %rd16, %rd17;
	// begin inline asm
	ld.ca.u64 %rd18, [%rd19];
	// end inline asm
	xor.b64  	%rd21, %rd18, %rd19;
	// begin inline asm
	ld.ca.u64 %rd20, [%rd21];
	// end inline asm
	xor.b64  	%rd23, %rd20, %rd21;
	// begin inline asm
	ld.ca.u64 %rd22, [%rd23];
	// end inline asm
	xor.b64  	%rd25, %rd22, %rd23;
	// begin inline asm
	ld.ca.u64 %rd24, [%rd25];
	// end inline asm
	xor.b64  	%rd27, %rd24, %rd25;
	// begin inline asm
	ld.ca.u64 %rd26, [%rd27];
	// end inline asm
	xor.b64  	%rd29, %rd26, %rd27;
	// begin inline asm
	ld.ca.u64 %rd28, [%rd29];
	// end inline asm
	xor.b64  	%rd31, %rd28, %rd29;
	// begin inline asm
	ld.ca.u64 %rd30, [%rd31];
	// end inline asm
	xor.b64  	%rd33, %rd30, %rd31;
	// begin inline asm
	ld.ca.u64 %rd32, [%rd33];
	// end inline asm
	xor.b64  	%rd35, %rd32, %rd33;
	// begin inline asm
	ld.ca.u64 %rd34, [%rd35];
	// end inline asm
	xor.b64  	%rd37, %rd34, %rd35;
	// begin inline asm
	ld.ca.u64 %rd36, [%rd37];
	// end inline asm
	xor.b64  	%rd39, %rd36, %rd37;
	// begin inline asm
	ld.ca.u64 %rd38, [%rd39];
	// end inline asm
	xor.b64  	%rd41, %rd38, %rd39;
	// begin inline asm
	ld.ca.u64 %rd40, [%rd41];
	// end inline asm
	xor.b64  	%rd43, %rd40, %rd41;
	// begin inline asm
	ld.ca.u64 %rd42, [%rd43];
	// end inline asm
	xor.b64  	%rd45, %rd42, %rd43;
	// begin inline asm
	ld.ca.u64 %rd44, [%rd45];
	// end inline asm
	xor.b64  	%rd47, %rd44, %rd45;
	// begin inline asm
	ld.ca.u64 %rd46, [%rd47];
	// end inline asm
	xor.b64  	%rd49, %rd46, %rd47;
	// begin inline asm
	ld.ca.u64 %rd48, [%rd49];
	// end inline asm
	xor.b64  	%rd51, %rd48, %rd49;
	// begin inline asm
	ld.ca.u64 %rd50, [%rd51];
	// end inline asm
	xor.b64  	%rd53, %rd50, %rd51;
	// begin inline asm
	ld.ca.u64 %rd52, [%rd53];
	// end inline asm
	xor.b64  	%rd55, %rd52, %rd53;
	// begin inline asm
	ld.ca.u64 %rd54, [%rd55];
	// end inline asm
	xor.b64  	%rd57, %rd54, %rd55;
	// begin inline asm
	ld.ca.u64 %rd56, [%rd57];
	// end inline asm
	xor.b64  	%rd59, %rd56, %rd57;
	// begin inline asm
	ld.ca.u64 %rd58, [%rd59];
	// end inline asm
	xor.b64  	%rd61, %rd58, %rd59;
	// begin inline asm
	ld.ca.u64 %rd60, [%rd61];
	// end inline asm
	xor.b64  	%rd63, %rd60, %rd61;
	// begin inline asm
	ld.ca.u64 %rd62, [%rd63];
	// end inline asm
	xor.b64  	%rd65, %rd62, %rd63;
	// begin inline asm
	ld.ca.u64 %rd64, [%rd65];
	// end inline asm
	xor.b64  	%rd67, %rd64, %rd65;
	// begin inline asm
	ld.ca.u64 %rd66, [%rd67];
	// end inline asm
	xor.b64  	%rd69, %rd66, %rd67;
	// begin inline asm
	ld.ca.u64 %rd68, [%rd69];
	// end inline asm
	xor.b64  	%rd71, %rd68, %rd69;
	// begin inline asm
	ld.ca.u64 %rd70, [%rd71];
	// end inline asm
	xor.b64  	%rd73, %rd70, %rd71;
	// begin inline asm
	ld.ca.u64 %rd72, [%rd73];
	// end inline asm
	xor.b64  	%rd75, %rd72, %rd73;
	// begin inline asm
	ld.ca.u64 %rd74, [%rd75];
	// end inline asm
	xor.b64  	%rd77, %rd74, %rd75;
	// begin inline asm
	ld.ca.u64 %rd76, [%rd77];
	// end inline asm
	xor.b64  	%rd79, %rd76, %rd77;
	// begin inline asm
	ld.ca.u64 %rd78, [%rd79];
	// end inline asm
	xor.b64  	%rd81, %rd78, %rd79;
	// begin inline asm
	ld.ca.u64 %rd80, [%rd81];
	// end inline asm
	xor.b64  	%rd83, %rd80, %rd81;
	// begin inline asm
	ld.ca.u64 %rd82, [%rd83];
	// end inline asm
	xor.b64  	%rd85, %rd82, %rd83;
	// begin inline asm
	ld.ca.u64 %rd84, [%rd85];
	// end inline asm
	xor.b64  	%rd87, %rd84, %rd85;
	// begin inline asm
	ld.ca.u64 %rd86, [%rd87];
	// end inline asm
	xor.b64  	%rd89, %rd86, %rd87;
	// begin inline asm
	ld.ca.u64 %rd88, [%rd89];
	// end inline asm
	xor.b64  	%rd91, %rd88, %rd89;
	// begin inline asm
	ld.ca.u64 %rd90, [%rd91];
	// end inline asm
	xor.b64  	%rd93, %rd90, %rd91;
	// begin inline asm
	ld.ca.u64 %rd92, [%rd93];
	// end inline asm
	xor.b64  	%rd95, %rd92, %rd93;
	// begin inline asm
	ld.ca.u64 %rd94, [%rd95];
	// end inline asm
	xor.b64  	%rd97, %rd94, %rd95;
	// begin inline asm
	ld.ca.u64 %rd96, [%rd97];
	// end inline asm
	xor.b64  	%rd99, %rd96, %rd97;
	// begin inline asm
	ld.ca.u64 %rd98, [%rd99];
	// end inline asm
	xor.b64  	%rd101, %rd98, %rd99;
	// begin inline asm
	ld.ca.u64 %rd100, [%rd101];
	// end inline asm
	xor.b64  	%rd103, %rd100, %rd101;
	// begin inline asm
	ld.ca.u64 %rd102, [%rd103];
	// end inline asm
	xor.b64  	%rd105, %rd102, %rd103;
	// begin inline asm
	ld.ca.u64 %rd104, [%rd105];
	// end inline asm
	xor.b64  	%rd107, %rd104, %rd105;
	// begin inline asm
	ld.ca.u64 %rd106, [%rd107];
	// end inline asm
	xor.b64  	%rd109, %rd106, %rd107;
	// begin inline asm
	ld.ca.u64 %rd108, [%rd109];
	// end inline asm
	xor.b64  	%rd111, %rd108, %rd109;
	// begin inline asm
	ld.ca.u64 %rd110, [%rd111];
	// end inline asm
	xor.b64  	%rd113, %rd110, %rd111;
	// begin inline asm
	ld.ca.u64 %rd112, [%rd113];
	// end inline asm
	xor.b64  	%rd115, %rd112, %rd113;
	// begin inline asm
	ld.ca.u64 %rd114, [%rd115];
	// end inline asm
	xor.b64  	%rd117, %rd114, %rd115;
	// begin inline asm
	ld.ca.u64 %rd116, [%rd117];
	// end inline asm
	xor.b64  	%rd119, %rd116, %rd117;
	// begin inline asm
	ld.ca.u64 %rd118, [%rd119];
	// end inline asm
	xor.b64  	%rd121, %rd118, %rd119;
	// begin inline asm
	ld.ca.u64 %rd120, [%rd121];
	// end inline asm
	xor.b64  	%rd123, %rd120, %rd121;
	// begin inline asm
	ld.ca.u64 %rd122, [%rd123];
	// end inline asm
	xor.b64  	%rd125, %rd122, %rd123;
	// begin inline asm
	ld.ca.u64 %rd124, [%rd125];
	// end inline asm
	xor.b64  	%rd127, %rd124, %rd125;
	// begin inline asm
	ld.ca.u64 %rd126, [%rd127];
	// end inline asm
	xor.b64  	%rd129, %rd126, %rd127;
	// begin inline asm
	ld.ca.u64 %rd128, [%rd129];
	// end inline asm
	cvt.u32.u64 	%r10, %rd128;
	add.s32 	%r16, %r16, %r10;
	add.s32 	%r15, %r15, 1;
	setp.ne.s32 	%p1, %r15, 8192;
	@%p1 bra 	$L__BB33_1;

	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	shl.b32 	%r12, %r14, 10;
	add.s32 	%r11, %r12, %r13;
	setp.eq.s32 	%p2, %r16, %r11;
	@%p2 bra 	$L__BB33_4;

	cvta.to.global.u64 	%rd131, %rd1;
	cvt.s64.s32 	%rd132, %r16;
	st.global.u64 	[%rd131], %rd132;

$L__BB33_4:
	ret;

}
	// .globl	_Z11stream_copyIdEvPT_S1_m
.visible .entry _Z11stream_copyIdEvPT_S1_m(
	.param .u64 _Z11stream_copyIdEvPT_S1_m_param_0,
	.param .u64 _Z11stream_copyIdEvPT_S1_m_param_1,
	.param .u64 _Z11stream_copyIdEvPT_S1_m_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<10>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd6, [_Z11stream_copyIdEvPT_S1_m_param_0];
	ld.param.u64 	%rd7, [_Z11stream_copyIdEvPT_S1_m_param_1];
	ld.param.u64 	%rd8, [_Z11stream_copyIdEvPT_S1_m_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r9, %r6, %r1, %r7;
	cvt.u64.u32 	%rd12, %r9;
	setp.ge.u64 	%p1, %rd12, %rd8;
	@%p1 bra 	$L__BB34_3;

	mov.u32 	%r8, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r8;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd6;

$L__BB34_2:
	shl.b64 	%rd9, %rd12, 3;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.f64 	%fd1, [%rd10];
	add.s64 	%rd11, %rd3, %rd9;
	st.global.f64 	[%rd11], %fd1;
	add.s32 	%r9, %r9, %r3;
	cvt.u64.u32 	%rd12, %r9;
	setp.lt.u64 	%p2, %rd12, %rd8;
	@%p2 bra 	$L__BB34_2;

$L__BB34_3:
	ret;

}

